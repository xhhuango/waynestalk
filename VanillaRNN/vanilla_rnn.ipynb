{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.593233Z",
     "start_time": "2025-02-13T06:23:10.525785Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.602045Z",
     "start_time": "2025-02-13T06:23:10.600064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Implements the tanh function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: (ndarray of any shape) or (scalar) - input to the tanh function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: (ndarray of same shape as Z) or (scalar) - output from the tanh function\n",
    "    \"\"\"\n",
    "\n",
    "    a = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: (ndarray of same shape as Z) - output of the activation function\n",
    "    \"\"\"\n",
    "\n",
    "    # Subtracting the maximum value in each column for numerical stability to avoid overflow\n",
    "    z_stable = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(z_stable)\n",
    "    a = exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    return a"
   ],
   "id": "1d02e8dffa25a0f6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.609892Z",
     "start_time": "2025-02-13T06:23:10.607267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rnn_cell_forward(xt, at_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xt: (ndarray (n_x, m)) - input data at timestep \"t\"\n",
    "    at_prev: (ndarray (n_a, m)) - hidden state at timestep \"t-1\"\n",
    "    parameters:\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state at_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input xt\n",
    "        Wya: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    at: (ndarray (n_a, m)) - hidden state at timestep \"t\"\n",
    "    yt: (ndarray (n_y, m)) - prediction at timestep \"t\"\n",
    "    cache: (tuple) - returning (at, at_prev, xt, zxt, y_hat_t, zyt) for the backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    Wyx = parameters[\"Wya\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    zxt = Waa @ at_prev + Wax @ xt + ba\n",
    "    at = np.tanh(zxt)\n",
    "    zyt = Wyx @ at + by\n",
    "    y_hat_t = softmax(zyt)\n",
    "\n",
    "    cache = (at, at_prev, xt, zxt, y_hat_t, zyt)\n",
    "    return at, y_hat_t, cache\n",
    "\n",
    "\n",
    "def rnn_forward(X, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation of the RNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (n_x, m, T_x)) - input data for each timestep\n",
    "    a0: (ndarray (n_a, m)) - initial hidden state\n",
    "    parameters:\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state at_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input xt\n",
    "        Wyx: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: (ndarray (n_a, m, T_x)) - hidden states for each timestep\n",
    "    y_hat: (ndarray (n_y, m, T_x)) - predictions for each timestep\n",
    "    caches: (tuple) - returning (list of cache, x) for the backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    n_x, m, T_x = X.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_hat = np.zeros((n_y, m, T_x))\n",
    "\n",
    "    at_prev = a0\n",
    "    for t in range(T_x):\n",
    "        at_prev, y_hat_t, cache = rnn_cell_forward(X[:, :, t], at_prev, parameters)\n",
    "        a[:, :, t] = at_prev\n",
    "        y_hat[:, :, t] = y_hat_t\n",
    "        caches.append(cache)\n",
    "\n",
    "    return a, y_hat, caches"
   ],
   "id": "7874588b533dfe6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.658879Z",
     "start_time": "2025-02-13T06:23:10.655960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rnn_cell_backward(y, dat, cache, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single backward step of the RNN-cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: (ndarray (n_y, m)) - true labels at timestep \"t\"\n",
    "    dat: (ndarray (n_a, m)) - gradient of the hidden state at timestep \"t\"\n",
    "    cache: (tuple) - (at, at_prev, xt, zxt, y_hat_t, zyt)\n",
    "    parameters:\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state at_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input xt\n",
    "        Wya: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: (dict) - the gradients\n",
    "        dWaa: (ndarray (n_a, n_a)) - gradient of the weight matrix multiplying the hidden state at_prev\n",
    "        dWax: (ndarray (n_a, n_x)) - gradient of the weight matrix multiplying the input xt\n",
    "        dWya: (ndarray (n_y, n_a)) - gradient of the weight matrix relating the hidden-state to the output\n",
    "        dba: (ndarray (n_a, 1)) - gradient of the bias\n",
    "        dby: (ndarray (n_y, 1)) - gradient of the bias\n",
    "        dat: (ndarray (n_a, m)) - gradient of the hidden state\n",
    "    \"\"\"\n",
    "\n",
    "    at, at_prev, xt, zt, y_hat_t, zyt = cache\n",
    "    dy = y_hat_t - y\n",
    "    gradients = {\n",
    "        \"dWya\": dy @ at.T,\n",
    "        \"dby\": np.sum(dy, axis=1, keepdims=True),\n",
    "    }\n",
    "    da = parameters[\"Wya\"].T @ dy + dat\n",
    "    dz = (1 - at ** 2) * da\n",
    "    gradients[\"dba\"] = np.sum(dz, axis=1, keepdims=True)\n",
    "    gradients[\"dWax\"] = dz @ xt.T\n",
    "    gradients[\"dWaa\"] = dz @ at_prev.T\n",
    "    gradients[\"dat\"] = parameters[\"Waa\"].T @ dz\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, caches):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of the RNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (n_x, m, T_x)) - input data\n",
    "    Y: (ndarray (n_y, m, T_x)) - true labels\n",
    "    parameters:\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state at_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input xt\n",
    "        Wya: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "    caches: (list) - list of caches from rnn_forward\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: (dict) - the gradients\n",
    "        dWaa: (ndarray (n_a, n_a)) - gradient of the weight matrix multiplying the hidden state at_prev\n",
    "        dWax: (ndarray (n_a, n_x)) - gradient of the weight matrix multiplying the input xt\n",
    "        dWya: (ndarray (n_y, n_a)) - gradient of the weight matrix relating the hidden-state to the output\n",
    "        dba: (ndarray (n_a, 1)) - gradient of the bias\n",
    "        dby: (ndarray (n_y, 1)) - gradient of the bias\n",
    "        dat: (ndarray (n_a, m)) - gradient of the hidden state\n",
    "    \"\"\"\n",
    "\n",
    "    n_x, m, T_x = X.shape\n",
    "    a1, a0, x1, zx1, y_hat1, zy1 = caches[0]\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "    gradients = {\n",
    "        \"dWaa\": np.zeros_like(Waa), \"dWax\": np.zeros_like(Wax), \"dWya\": np.zeros_like(Wya),\n",
    "        \"dba\": np.zeros_like(ba), \"dby\": np.zeros_like(by), \"dat\": np.zeros_like(a0),\n",
    "    }\n",
    "\n",
    "    dat = np.zeros_like(a0)\n",
    "    for t in reversed(range(T_x)):\n",
    "        grads = rnn_cell_backward(Y[:, :, t], dat, caches[t], parameters)\n",
    "        for key in gradients:\n",
    "            gradients[key] += grads[key]\n",
    "        dat = grads[\"dat\"]\n",
    "\n",
    "    return gradients"
   ],
   "id": "891b23aecf68adb7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.663920Z",
     "start_time": "2025-02-13T06:23:10.662549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_hat: (ndarray (n_y, m, T_x)) - predictions for each timestep\n",
    "    Y: (ndarray (n_y, m, T_x)) - true labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: (float) - the cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    return -np.sum(Y * np.log(Y_hat))"
   ],
   "id": "ecbde4e5e85da240",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.696341Z",
     "start_time": "2025-02-13T06:23:10.693989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clip(gradients, max_value):\n",
    "    \"\"\"\n",
    "    Clips the gradients to a maximum value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gradients: (dict) - the gradients\n",
    "        dWaa: (ndarray (n_a, n_a)) - gradient of the weight matrix multiplying the hidden state at_prev\n",
    "        dWax: (ndarray (n_a, n_x)) - gradient of the weight matrix multiplying the input xt\n",
    "        dWya: (ndarray (n_y, n_a)) - gradient of the weight matrix relating the hidden-state to the output\n",
    "        dba: (ndarray (n_a, 1)) - gradient of the bias\n",
    "        dby: (ndarray (n_y, 1)) - gradient of the bias\n",
    "    max_value: (float) - the maximum value to clip the gradients\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: (dict) - the clipped gradients\n",
    "    \"\"\"\n",
    "\n",
    "    dWaa, dWax, dWya = gradients[\"dWaa\"], gradients[\"dWax\"], gradients[\"dWya\"]\n",
    "    dba, dby = gradients[\"dba\"], gradients[\"dby\"]\n",
    "\n",
    "    for gradient in [dWax, dWaa, dWya, dba, dby]:\n",
    "        np.clip(gradient, -max_value, max_value, out=gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the parameters using the gradients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: (dict) - the parameters\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state at_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input xt\n",
    "        Wya: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "    gradients: (dict) - the gradients\n",
    "        dWaa: (ndarray (n_a, n_a)) - gradient of the weight matrix multiplying the hidden state at_prev\n",
    "        dWax: (ndarray (n_a, n_x)) - gradient of the weight matrix multiplying the input xt\n",
    "        dWya: (ndarray (n_y, n_a)) - gradient of the weight matrix relating the hidden-state to the output\n",
    "        dba: (ndarray (n_a, 1)) - gradient of the bias\n",
    "        dby: (ndarray (n_y, 1)) - gradient of the bias\n",
    "    learning_rate: (float) - the learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    parameters[\"Waa\"] -= learning_rate * gradients[\"dWaa\"]\n",
    "    parameters[\"Wax\"] -= learning_rate * gradients[\"dWax\"]\n",
    "    parameters[\"Wya\"] -= learning_rate * gradients[\"dWya\"]\n",
    "    parameters[\"ba\"] -= learning_rate * gradients[\"dba\"]\n",
    "    parameters[\"by\"] -= learning_rate * gradients[\"dby\"]"
   ],
   "id": "4250a5394764c97c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.704702Z",
     "start_time": "2025-02-13T06:23:10.702809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate):\n",
    "    \"\"\"\n",
    "    Implements the forward and backward propagation of the RNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (n_x, m, T_x)) - the input data\n",
    "    Y: (ndarray (n_y, m, T_x)) - true labels\n",
    "    a_prev: (ndarray (n_a, m)) - the initial hidden state\n",
    "    parameters: (dict) - the initial parameters\n",
    "        Waa: (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state A_prev\n",
    "        Wax: (ndarray (n_a, n_x)) - weight matrix multiplying the input Xt\n",
    "        Wya: (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        ba: (ndarray (n_a, 1)) - bias\n",
    "        by: (ndarray (n_y, 1)) - bias\n",
    "    learning_rate: (float) - the learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: (float) - the cross-entropy loss\n",
    "    gradients: (dict) - the gradients\n",
    "    a: (ndarray (n_a, m)) - the hidden state at the last timestep\n",
    "    \"\"\"\n",
    "\n",
    "    a, Y_hat, caches = rnn_forward(X, a_prev, parameters)\n",
    "    loss = compute_loss(Y_hat, Y)\n",
    "    gradients = rnn_backward(X, Y, parameters, caches)\n",
    "    gradients = clip(gradients, 5)\n",
    "    update_parameters(parameters, gradients, learning_rate)\n",
    "    return loss, gradients, a[:, :, -1]"
   ],
   "id": "3139abddeb5a6a54",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.712175Z",
     "start_time": "2025-02-13T06:23:10.710419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initializes the parameters for the RNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_a: (int) - number of units in the hidden state\n",
    "    n_x: (int) - number of units in the input data\n",
    "    n_y: (int) - number of units in the output data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: (dict) - initialized parameters\n",
    "        \"Wax\": (ndarray (n_a, n_x)) - weight matrix multiplying the input\n",
    "        \"Waa\": (ndarray (n_a, n_a)) - weight matrix multiplying the hidden state\n",
    "        \"Wya\": (ndarray (n_y, n_a)) - weight matrix relating the hidden-state to the output\n",
    "        \"ba\": (ndarray (n_a, 1)) - bias\n",
    "        \"by\": (ndarray (n_y, 1)) - bias\n",
    "    \"\"\"\n",
    "\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01\n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01\n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01\n",
    "    ba = np.zeros((n_a, 1))\n",
    "    by = np.zeros((n_y, 1))\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "    return parameters"
   ],
   "id": "84dcaa4f8ef5c351",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.717010Z",
     "start_time": "2025-02-13T06:23:10.714803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = \"\\nabcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "index_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def rnn_model(inputs, num_iterations, learning_rate):\n",
    "    m = 1\n",
    "    n_a = 50\n",
    "    n_x = n_y = len(chars)\n",
    "    a_prev = np.zeros((n_a, m))\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "\n",
    "    for j in range(num_iterations):\n",
    "        index = j % len(inputs)\n",
    "        example = inputs[index]\n",
    "        example_indexes = [char_to_index[c] for c in example]\n",
    "\n",
    "        T_x = len(example_indexes) + 1\n",
    "        X = np.zeros((n_x, m, T_x))\n",
    "        Y = np.zeros((n_y, m, T_x))\n",
    "        for i, idx in enumerate([None] + example_indexes):\n",
    "            if idx:\n",
    "                X[idx, m - 1, i] = 1\n",
    "        for i, idx in enumerate(example_indexes + [char_to_index[\"\\n\"]]):\n",
    "            Y[idx, m - 1, i] = 1\n",
    "\n",
    "        loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate)\n",
    "\n",
    "    return parameters"
   ],
   "id": "a027a9e072f31a14",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:10.724935Z",
     "start_time": "2025-02-13T06:23:10.722681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample(parameters, char_to_ix, ix_to_char):\n",
    "    n_x = parameters[\"by\"].shape[0]\n",
    "    n_a = parameters[\"Waa\"].shape[1]\n",
    "    x = np.zeros((n_x, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    newline_char = char_to_ix[\"\\n\"]\n",
    "\n",
    "    while idx != newline_char and len(indices) != 50:\n",
    "        a_prev, y_hat, cache = rnn_cell_forward(x, a_prev, parameters)\n",
    "        idx = np.random.choice(list(range(n_x)), p=y_hat.ravel())\n",
    "        indices.append(idx)\n",
    "        x = np.zeros((n_x, 1))\n",
    "        x[idx] = 1\n",
    "\n",
    "    if indices[-1] != newline_char:\n",
    "        indices.append(char_to_ix[\"\\n\"])\n",
    "\n",
    "    text = ''.join(ix_to_char[ix] for ix in indices)\n",
    "    return text"
   ],
   "id": "35cb104e9670e01f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T06:23:19.525484Z",
     "start_time": "2025-02-13T06:23:10.728074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = open(\"dinos.txt\", \"r\").read().split(\"\\n\")\n",
    "data = [x.strip().lower() for x in data]\n",
    "parameters = rnn_model(data, 22001, 0.01)\n",
    "text = sample(parameters, char_to_index, index_to_char)\n",
    "text"
   ],
   "id": "c21580c1db5d137b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'werrosaurus\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
