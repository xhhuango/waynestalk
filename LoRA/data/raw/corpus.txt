Published as a conference paper at ICLR 2025
FORGETTING TRANSFORMER : SOFTMAX ATTENTION WITH A FORGET GATE
Zhixuan Lin∗
Mila & Universit´e de Montr´eal
zxlin.cs@gmail.com
Evgenii Nikishin
Mila & Universit´e de Montr´eal
evgenii.nikishin@mila.quebec
Xu Owen He†
MakerMaker AI
owen.hexu@gmail.com
Aaron Courville
Mila & Universit´e de Montr´eal
courvila@mila.quebec
ABSTRACT
An essential component of modern recurrent sequence models is the forget gate.
While Transformers do not have an explicit recurrent form, we show that a
forget gate can be naturally incorporated into Transformers by down-weighting
the unnormalized attention scores in a data-dependent way. We name this at-
tention mechanism Forgetting Attention and the resulting model the Forgetting
Transformer (FoX). We show that FoX outperforms the Transformer on long-
context language modeling, length extrapolation, and short-context downstream
tasks, while performing on par with the Transformer on long-context down-
stream tasks. Moreover, it is compatible with the FlashAttention algorithm and
does not require any positional embeddings. Several analyses, including the
needle-in-the-haystack test, show that FoX also retains the Transformer’s supe-
rior long-context capabilities over recurrent sequence models such as Mamba-
2, HGRN2, and DeltaNet. We also introduce a “Pro” block design that incor-
porates some common architectural components in recurrent sequence models
and find it significantly improves the performance of both FoX and the Trans-
former. Our code is available at https://github.com/zhixuan-lin/
forgetting-transformer.
1 I NTRODUCTION
Despite the growing interest in reviving recurrent sequence models (Gu et al., 2021; Peng et al.,
2021; Yang et al., 2023; Gu & Dao, 2023; Sun et al., 2023; De et al., 2024; Qin et al., 2024b; Dao &
Gu, 2024; Peng et al., 2024; Beck et al., 2024; Zhang et al., 2024), these models still underperform
the Transformer (Vaswani et al., 2017) in terms of long-context capabilities (Hsieh et al., 2024;
Waleffe et al., 2024; Shen et al., 2024; Qin et al., 2024a), likely due to their relatively small fixed-
sized hidden states (Jelassi et al., 2024). While the Transformer excels in handling long-context
information, it lacks an explicit mechanism for forgetting past information in adata-dependent way.
Such a mechanism – often implemented as some form of the forget gate (Gers et al., 2000) – is
ubiquitous in recurrent sequence models and has proven critical in their success in short-context
tasks (Greff et al., 2016; Van Der Westhuizen & Lasenby, 2018; Peng et al., 2021; Yang et al., 2023;
Gu & Dao, 2023). A natural question to ask is then: can we have a forget gate in Transformers?
To address this question, we leverage an important fact: many recurrent sequence models with a
forget gate can be written in a parallel linear attention form (Katharopoulos et al., 2020) analogous
to softmax attention (Yang et al., 2023; Dao & Gu, 2024). In this parallel form, the forget gate
mechanism translates into down-weighing the unnormalized attention scores in a data-dependent
way. Our key insight is that this exact mechanism is also applicable to softmax attention. We name
this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer
(FoX).
∗Correspondence to Zhixuan Lin.
†Work done while at Google DeepMind.
1
arXiv:2503.02130v2  [cs.LG]  31 Mar 2025
Published as a conference paper at ICLR 2025
We show that FoX outperforms the Transformer on long-context language modeling, length ex-
trapolation, and short-context downstream tasks, while performing on par with the Transformer on
long-context downstream tasks. Notably, it does not require any positional embeddings. It also re-
tains Transformers’ long-context retrieval abilities and achieves near-perfect accuracy in the needle-
in-the-haystack test (Kamradt, 2023) within the training context length. In contrast, all the tested
recurrent sequence models fail. We also introduce a “Pro” block design that integrates several archi-
tectural components commonly used in recurrent sequence models, which significantly improves the
performance of FoX and the baseline Transformer. Finally, we show that FoX can be implemented
in a hardware-aware way with a simple modification to the FlashAttention (Dao, 2023) algorithm.
2 B ACKGROUND : L INEAR ATTENTION WITH A FORGET GATE
This section introduces the notation used in this work and gives a brief background on linear at-
tention. We also introduce a gated variant of linear attention and discuss its parallel form, which
naturally leads to FoX. Throughout this work, we only consider causal sequence modeling.
2.1 L INEAR ATTENTION
Standard causal softmax attention takes a sequence of input vectors(xi)L
i=1 and produces a sequence
of output vectors (oi)L
i=1, where xi, oi ∈ Rd, i∈ {1, . . . , L}. Each oi is computed as follows:
qi, ki, vi = Wqxi,Wkxi, Wvxi ∈ Rd, (1)
oi =
Pi
j=1 kexp(qi, kj)vj
Pi
j=1 kexp(qi, kj)
=
Pi
j=1 exp(q⊤
i kj)vj
Pi
j=1 exp(q⊤
i kj)
, (2)
where Wq, Wk, Wv ∈ Rd×d are projection matrices andkexp(q, k) = exp(q⊤k) is the exponential
dot product kernel.1
Linear attention (Katharopoulos et al., 2020) replaces the exponential dot product kernel
kexp(q, k) = exp(q⊤k) with a kernel kϕ(q, k) with some feature representation ϕ : Rd → (R+)d′
:
oi =
Pi
j=1 kϕ(qi, kj)vj
Pi
j=1 kϕ(qi, kj)
=
Pi
j=1(ϕ(qi)⊤ϕ(kj))vj
Pi
j=1 ϕ(qi)⊤ϕ(kj)
(3)
Following Yang et al. (2023), we call this theparallel form of linear attention as it can be computed
with matrix multiplications. Alternatively, linear attention can be computed in a recurrent form:
St = St−1 + vtϕ(kt)⊤ (4)
zt = zt−1 + ϕ(kt) (5)
ot = Stϕ(qt)
z⊤
t ϕ(qt), (6)
where St ∈ Rd×d′
, zt ∈ Rd′
, t∈ {0, . . . , L} are computed recurrently, with S0 = 0 and zt = 0.
2.2 L INEAR ATTENTION WITH A FORGET GATE
The recurrent form of linear attention makes it natural to introduce a forget gate. Specifically, we
can compute a scalar forget gate ft = σ(w⊤
f xt + bf ) ∈ R at each timestep, where σ is the sigmoid
function and wf ∈ Rd, bf ∈ R are learnable parameters. The recurrent computation is then:
St = ftSt−1 + vtϕ(kt)⊤ (7)
zt = ftzt−1 + ϕ(kt) (8)
ot = Stϕ(qt)
z⊤
t ϕ(qt). (9)
1Note we omit the 1√
d scaling factor to reduce visual clutter. In practice we always scale q⊤
i kj by 1√
d .
2
Published as a conference paper at ICLR 2025
Note that this gated variant of linear attention differs from most models in the literature. In particular,
most gated variants of linear attention models, such as GLA (Yang et al., 2023) and Mamba-2 (Dao
& Gu, 2024), do not have the normalization term (i.e., there is no zt, and the output is just ot =
Stϕ(qt)). We keep the normalization term to maintain similarity with softmax attention.
Crucially, similar to the normalization-free version derived in GLA and Mamba-2, we can show that
this gated variant of linear attention also has a parallel form:
oi =
Pi
j=1 Fijϕ(qi)⊤ϕ(kj)vj
Pi
j=1 Fijϕ(qi)⊤ϕ(kj)
=
Pi
j=1 Fijkϕ(qi, kj)vj
Pi
j=1 Fijkϕ(qi, kj)
, (10)
where Fij = Qi
l=j+1 fl, with the convention that Fij = 1 if i = j. Our key observation is that
Equation 10 and the softmax attention in Equation 2 are very similar in form. In fact, if we change
the kernel kϕ in Equation 10 back to the exponential dot product kernel kexp, we obtain softmax
attention with a forget gate. We introduce this formally in the next section.
3 F ORGETTING TRANSFORMER
Our proposed model, the Forgetting Transformer (FoX), features a modified softmax attention
mechanism with a forget gate. We name this attention mechanism Forgetting Attention. Similar
to the gated variant of linear attention introduced in the previous section, we first compute a scalar
forget gate ft = σ(w⊤
f xt + bf ) ∈ R for each timestep t. The output of the attention is then
oi =
Pi
j=1 Fij exp(q⊤
i kj)vj
Pi
j=1 Fij exp(q⊤
i kj)
=
Pi
j=1 exp(q⊤
i kj + Dij)vj
Pi
j=1 exp(q⊤
i kj + Dij)
, (11)
where Fij = Qi
l=j+1 fl and Dij = logFij = Pi
l=j+1 log fl. This can be written in matrix form:
D = logF ∈ RL×L, (12)
O = softmax(QK⊤ + D)V ∈ RL×d, (13)
where F ∈ RL×L is a lower triangular matrix whose non-zero entries are Fij, i.e., Fij = Fij
if i ≥ j and 0 otherwise. We adopt the convention that log 0 =−∞. Q, K, V , O ∈ RL×d are
matrices containing qi, ki, vi, oi, i∈ {1, . . . , L} as the rows. Thesoftmax operation is applied row-
wise. For multi-head attention with H heads, we maintain H instances of forget gate parameters
{w(h)
f }H
h=1 and {b(h)
f }H
h=1 and compute the forget gate values {f(h)
t }H
h=1 separately for each head.
Hardware-aware implementation The logit bias form on the rightmost side of Equation 11 can
be computed with a simple modification to the FlashAttention (Dao, 2023) algorithm. Here we
briefly describe the forward pass. The backward pass follows a similar idea. First, we compute the
cumulative sum ci = Pi
l=1 log fl for i ∈ {1, . . . , L} and store it in the high-bandwidth memory
(HBM) of the GPU. This allows us to compute Dij = ci − cj easily later. Whenever we compute
the attention logit q⊤
i kj in the GPU’s fast shared memory (SRAM) (as in FlashAttention), we also
load ci and cj to SRAM, compute Dij, and add it to the attention logit. The rest of the forward pass
remains the same as FlashAttention. This algorithm avoids instantiating the L × L D matrix in the
HBM. We provide a detailed algorithm description in Appendix E. Moreover, since the forget gates
are scalars instead of vectors, the additional computation and parameters introduced are negligible.
Connection to ALiBi Besides its natural connection to gated linear attention, Forgetting Attention
can also be seen as a data-dependent and learnable version of ALiBi (Press et al., 2021). ALiBi
applies a data-independent bias bij = −(i − j)mh to the attention logits, where mh is a fixed slope
specific to each head h. It is easy to show that ALiBi is equivalent to using a fixed, head-specific,
and data-independent forget gate f(h)
t = exp(−mh). In Section 4.5, we verify the superiority of
data-dependent forget gates over ALiBi.
Positional embeddings Though we find that using Rotary Position Embeddings (RoPE) (Su et al.,
2024) sometimes slightly improves the performance of FoX, it is not necessary as it is for the stan-
dard Transformer (see ablations in Section 4.5). For simplicity, we do not use RoPE or any other
positional embeddings for FoX by default.
3
Published as a conference paper at ICLR 2025
FoX Layer
RMSNormLinearShiftLinearShiftLinearLinearLinear
Forgetting Attention
RMSNormRMSNorm
RMSNorm
Linear
<latexit sha1_base64="ugFhzxFssn6b/vmOPW6Kd5dyrfU=">AAAB+XicbVBPS8MwHP11/pvzX9Wjl+AQPI1WZHocevE4wc3BVkqapltYmtYkHYyyb+LFgyJe/Sbe/DamWw+6+SDk8d7vR15ekHKmtON8W5W19Y3Nrep2bWd3b//APjzqqiSThHZIwhPZC7CinAna0Uxz2kslxXHA6WMwvi38xwmViiXiQU9T6sV4KFjECNZG8m17ECQ8VNPYXPnTzNe+XXcazhxolbglqUOJtm9/DcKEZDEVmnCsVN91Uu3lWGpGOJ3VBpmiKSZjPKR9QwWOqfLyefIZOjNKiKJEmiM0mqu/N3IcqyKcmYyxHqllrxD/8/qZjq69nIk001SQxUNRxpFOUFEDCpmkRPOpIZhIZrIiMsISE23KqpkS3OUvr5LuRcNtNpr3l/XWTVlHFU7gFM7BhStowR20oQMEJvAMr/Bm5daL9W59LEYrVrlzDH9gff4AWk6UJQ==</latexit>
q t
<latexit sha1_base64="t6m8VNgXPjxdLI/z3R5gZV4pMUs=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwVRKR6rLoxmUF+4A2hMlk0g6dzISZSaGE/okbF4q49U/c+TdO2iy09cAwh3PuZc6cMGVUadf9tiobm1vbO9Xd2t7+weGRfXzSVSKTmHSwYEL2Q6QIo5x0NNWM9FNJUBIy0gsn94XfmxKpqOBPepYSP0EjTmOKkTZSYNvDULBIzRJz5ZN5oAO77jbcBZx14pWkDiXagf01jATOEsI1Zkipgeem2s+R1BQzMq8NM0VShCdoRAaGcpQQ5eeL5HPnwiiREwtpDtfOQv29kaNEFeHMZIL0WK16hfifN8h0fOvnlKeZJhwvH4oz5mjhFDU4EZUEazYzBGFJTVYHj5FEWJuyaqYEb/XL66R71fCajebjdb11V9ZRhTM4h0vw4AZa8ABt6ACGKTzDK7xZufVivVsfy9GKVe6cwh9Ynz9RJJQf</latexit>
k t
<latexit sha1_base64="lmkYlEXv394WjiNGCeJ8Q40FDvo=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwVRKR6rLoxmUF+4A2hMlk0g6dzISZSaGE/okbF4q49U/c+TdO2iy09cAwh3PuZc6cMGVUadf9tiobm1vbO9Xd2t7+weGRfXzSVSKTmHSwYEL2Q6QIo5x0NNWM9FNJUBIy0gsn94XfmxKpqOBPepYSP0EjTmOKkTZSYNvDULBIzRJz5dN5oAO77jbcBZx14pWkDiXagf01jATOEsI1Zkipgeem2s+R1BQzMq8NM0VShCdoRAaGcpQQ5eeL5HPnwiiREwtpDtfOQv29kaNEFeHMZIL0WK16hfifN8h0fOvnlKeZJhwvH4oz5mjhFDU4EZUEazYzBGFJTVYHj5FEWJuyaqYEb/XL66R71fCajebjdb11V9ZRhTM4h0vw4AZa8ABt6ACGKTzDK7xZufVivVsfy9GKVe6cwh9Ynz9h8ZQq</latexit>
v t
<latexit sha1_base64="yLB1reonSHxg7kUfMj6QS0jBWtc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh7CP/XLFrbpzkFXi5aQCORr98ldvELM04gqZpMZ0PTdBP6MaBZN8WuqlhieUjemQdy1VNOLGz+anTsmZVQYkjLUthWSu/p7IaGTMJApsZ0RxZJa9mfif100xvPYzoZIUuWKLRWEqCcZk9jcZCM0ZyokllGlhbyVsRDVlaNMp2RC85ZdXSeui6tWqtfvLSv0mj6MIJ3AK5+DBFdThDhrQBAZDeIZXeHOk8+K8Ox+L1oKTzxzDHzifP1lijdw=</latexit>
f t
<latexit sha1_base64="tdy5cBUx22e49sInllEMc7AaEZY=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWrV2f1mp3+RxFOEETuEcAriCOtxBA5pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPn/GPLg==</latexit>
 
SwiGLU MLP
RMSNorm
<latexit sha1_base64="tdy5cBUx22e49sInllEMc7AaEZY=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWrV2f1mp3+RxFOEETuEcAriCOtxBA5pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPn/GPLg==</latexit>
 
FoX Layer
RMSNormLinearShiftLinearShiftLinearLinearLinear
Forgetting Attention
RMSNormRMSNorm
RMSNorm
Linear
<latexit sha1_base64="ugFhzxFssn6b/vmOPW6Kd5dyrfU=">AAAB+XicbVBPS8MwHP11/pvzX9Wjl+AQPI1WZHocevE4wc3BVkqapltYmtYkHYyyb+LFgyJe/Sbe/DamWw+6+SDk8d7vR15ekHKmtON8W5W19Y3Nrep2bWd3b//APjzqqiSThHZIwhPZC7CinAna0Uxz2kslxXHA6WMwvi38xwmViiXiQU9T6sV4KFjECNZG8m17ECQ8VNPYXPnTzNe+XXcazhxolbglqUOJtm9/DcKEZDEVmnCsVN91Uu3lWGpGOJ3VBpmiKSZjPKR9QwWOqfLyefIZOjNKiKJEmiM0mqu/N3IcqyKcmYyxHqllrxD/8/qZjq69nIk001SQxUNRxpFOUFEDCpmkRPOpIZhIZrIiMsISE23KqpkS3OUvr5LuRcNtNpr3l/XWTVlHFU7gFM7BhStowR20oQMEJvAMr/Bm5daL9W59LEYrVrlzDH9gff4AWk6UJQ==</latexit>
q t
<latexit sha1_base64="t6m8VNgXPjxdLI/z3R5gZV4pMUs=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwVRKR6rLoxmUF+4A2hMlk0g6dzISZSaGE/okbF4q49U/c+TdO2iy09cAwh3PuZc6cMGVUadf9tiobm1vbO9Xd2t7+weGRfXzSVSKTmHSwYEL2Q6QIo5x0NNWM9FNJUBIy0gsn94XfmxKpqOBPepYSP0EjTmOKkTZSYNvDULBIzRJz5ZN5oAO77jbcBZx14pWkDiXagf01jATOEsI1Zkipgeem2s+R1BQzMq8NM0VShCdoRAaGcpQQ5eeL5HPnwiiREwtpDtfOQv29kaNEFeHMZIL0WK16hfifN8h0fOvnlKeZJhwvH4oz5mjhFDU4EZUEazYzBGFJTVYHj5FEWJuyaqYEb/XL66R71fCajebjdb11V9ZRhTM4h0vw4AZa8ABt6ACGKTzDK7xZufVivVsfy9GKVe6cwh9Ynz9RJJQf</latexit>
k t
<latexit sha1_base64="lmkYlEXv394WjiNGCeJ8Q40FDvo=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwVRKR6rLoxmUF+4A2hMlk0g6dzISZSaGE/okbF4q49U/c+TdO2iy09cAwh3PuZc6cMGVUadf9tiobm1vbO9Xd2t7+weGRfXzSVSKTmHSwYEL2Q6QIo5x0NNWM9FNJUBIy0gsn94XfmxKpqOBPepYSP0EjTmOKkTZSYNvDULBIzRJz5dN5oAO77jbcBZx14pWkDiXagf01jATOEsI1Zkipgeem2s+R1BQzMq8NM0VShCdoRAaGcpQQ5eeL5HPnwiiREwtpDtfOQv29kaNEFeHMZIL0WK16hfifN8h0fOvnlKeZJhwvH4oz5mjhFDU4EZUEazYzBGFJTVYHj5FEWJuyaqYEb/XL66R71fCajebjdb11V9ZRhTM4h0vw4AZa8ABt6ACGKTzDK7xZufVivVsfy9GKVe6cwh9Ynz9h8ZQq</latexit>
v t
<latexit sha1_base64="yLB1reonSHxg7kUfMj6QS0jBWtc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh7CP/XLFrbpzkFXi5aQCORr98ldvELM04gqZpMZ0PTdBP6MaBZN8WuqlhieUjemQdy1VNOLGz+anTsmZVQYkjLUthWSu/p7IaGTMJApsZ0RxZJa9mfif100xvPYzoZIUuWKLRWEqCcZk9jcZCM0ZyokllGlhbyVsRDVlaNMp2RC85ZdXSeui6tWqtfvLSv0mj6MIJ3AK5+DBFdThDhrQBAZDeIZXeHOk8+K8Ox+L1oKTzxzDHzifP1lijdw=</latexit>
f t
<latexit sha1_base64="tdy5cBUx22e49sInllEMc7AaEZY=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWrV2f1mp3+RxFOEETuEcAriCOtxBA5pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPn/GPLg==</latexit>
 
SwiGLU MLP
RMSNorm
<latexit sha1_base64="tdy5cBUx22e49sInllEMc7AaEZY=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWrV2f1mp3+RxFOEETuEcAriCOtxBA5pA4BGe4RXePOW9eO/ex6K14OUzx/AH3ucPn/GPLg==</latexit>
 
Figure 1: Default architecture of FoX. ( left) A single FoX block. ( right) A single FoX (Pro) layer.
All RMSNorms on the right are applied independently to each head. σ is the sigmoid function. ⊗ is
element-wise multiplication. ShiftLinear implements the computation in Equation 14.
Architecture design We test FoX with two different architectures. First, we replace RoPE in
the LLaMA architecture (Touvron et al., 2023) with forget gates and refer to this model as FoX
(LLaMA). Second, we test an improved “Pro” architecture with output gates2 and output normaliza-
tion (also used in GLA and Mamba-2). We also use QK-norm (Dehghani et al., 2023) and apply a
simplified variant of data-dependent token shift (Peng et al., 2024) to the keys and values (KV-shift).
Concretely, the keys (ki)L
i=1 are computed as follows with additional parameters wk ∈ Rd:
˜kt = Wkxt ∈ Rd, α key
t = σ(w⊤
k xt) ∈ R
kt = RMSNorm(αkey
t ˜kt−1 + (1− αkey
t )˜kt)
(14)
The values (vi)L
i=1 are computed in the same way, but without RMSNorm. The overall architecture
is shown in Figure 1 and detailed in Appendix A. We refer to the resulting model as FoX (Pro).
4 E MPIRICAL STUDY
The advantages of Transformers in long-context abilities over recurrent sequence models have been
verified multiple times (Hsieh et al., 2024; Waleffe et al., 2024; Shen et al., 2024; Qin et al., 2024a).
However, forget gates introduce a recency bias. It is thus natural to ask whether FoX still maintains
this advantage. Therefore, our empirical study places a special focus on long-context capabilities.
4.1 E XPERIMENTAL SETUP
Dataset and baselines We focus on long-context language modeling and train all models on
LongCrawl64 (Buckman, 2024), a long-sequence subset of RedPajama-v2 (Together Computer,
2023) pre-tokenized with the TikToken tokenizer (OpenAI, 2022) for GPT-2 (Radford et al., 2019).
For baselines, we focus on two types of comparisons. First, we compare FoX with the Transformer.
For the Transformer, we also test both the LLaMA and the Pro architecture (referred to as Trans-
former (LLaMA) and Transformer (Pro), respectively). Similar to Xiong et al. (2023), we find it
crucial to use a large RoPE angle θ for the Transformer for long-context training. Following Xiong
et al. (2023) we use θ = 500000. Second, to show the advantage of FoX over recurrent sequence
models in long-context capabilities, we compare it with Mamba-2 (Dao & Gu, 2024), HGRN2 (Qin
et al., 2024a), and DeltaNet (Yang et al., 2024). The implementation of all models is based on the
Flash Linear Attention repository (Yang & Zhang, 2024).
Training setup For our main experiments, we train models with 760M (non-embedding) parame-
ters on a 45 ×230-token (roughly 48B tokens) subset of LongCrawl64 with a training context length
2When output gates are used, we reduce the number of parameters in the MLPs so the total number of
parameters remains the same.
4
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.50
1.55
1.60
1.65Loss L(i)
0 8k 16k 24k 32k 40k 48k 56k 64k
Validation context length l
4.4
4.6
4.8
5.0
5.2
5.4Perplexity P(l)
FoX (Pro)
FoX (LLaMA)
Transformer (Pro)
Transformer (LLaMA)
Mamba-2
HGRN2
DeltaNet
Figure 2: ( left) Per-token loss L(i) at different token position i. ( right) Validation perplexity P(l)
over different validation context length l. The vertical dashed line indicates the training context
length. The per-token loss is smoothed using a moving average sliding window of 101 tokens.
of 16384 tokens. For the validation set, we use a 2 × 230-token subset of the LongCrawl64 held-out
set with sequences of 65536 tokens. We choose a longer validation context length than the training
context length to test the length extrapolation abilities of the models. All models are trained with
AdamW (Loshchilov, 2017) with(β1, β2) = (0.9, 0.95). We use a linear learning rate warmup from
0 to the peak learning rate for the first 256 × 220 tokens and then a cosine decay to 0. Each training
batch contains 0.5 × 220 tokens. All models use a weight decay of 0.1 and gradient clipping of 1.0.
We search the learning rate for each model within {1 × 10i, 2 × 10i, 5 × 10i} for different i’s until
we identify a locally optimal value. We tune the head dimensions for FoX and the Transformer in
{64, 128}. We find thatFoX often prefers higher learning rates and more heads/smaller head dimen-
sions than the Transformer, and the Pro models often prefer higher learning rates than the LLaMA
models. Details of the hyperparameters and experimental setup can be found in Appendix B.
4.2 L ONG -CONTEXT LANGUAGE MODELING
Metrics For our main metric, we use per-token loss on the validation set at different token posi-
tions. To be precise, let V be the vocabulary size, y(j)
i ∈ {0, 1}V be the one-hot vector encoding
the language modeling target for the i-th token in the j-th validation sequence, and p(j)
i ∈ RV be
the corresponding output probabilities of the model, then the per-token loss L(i) at token position i
is defined as L(i) = 1
M
PM
j=1 −log[(p(j)
i )⊤y(j)
i ], where M is the number of validation sequences.
The per-token loss is particularly meaningful for understanding the long-context capabilities of a
model. Informally, a monotonically decreasing L(i) with a steep slope indicates the model is using
the full context well. On the other hand, if L(i) plateaus after some token position k, it indicates
the model struggles to use tokens that are k tokens away from the current token position for its
prediction. This correspondence between the slope of L(i) and the model’s context utilization is
explained in more detail in Appendix C.
Besides per-token loss, we also report perplexity over different context lengths. Concretely, perplex-
ity P(l) over a context length l is defined as P(l) = exp(1
l
Pl
i=1 L(i)). We warn the readers that
the slope of P(l) is less meaningful. Since P(l) is the exponential of the cumulative average ofL(i),
even if L(i) plateaus after some token position k, P(l) may still keep decreasing after k, giving the
wrong impression that the model can make use of the part of the context that is k tokens away.
Results In Figure 2, we show the per-token loss L(i) at different token indices i and perplexity
P(l) over different validation context lengths l. As shown in Figure 2, with either architecture,
FoX outperforms the standard Transformer both within and beyond (i.e., length extrapolation) the
5
Published as a conference paper at ICLR 2025
0k 5k 10k 15k
Key Position
0k
5k
10k
15kQuery Position
Layer 8, Head 2, F
0k 5k 10k 15k
Key Position
Layer 8, Head 2, A
0k 5k 10k 15k
Key Position
0k
5k
10k
15kQuery Position
Layer 16, Head 4, F
0k 5k 10k 15k
Key Position
Layer 16, Head 4, A
0.0 0.2 0.4 0.6 0.8 1.0
Score
Figure 3: Visualization of the forget gate weight matrix F and the attention score matrix A from
two heads in different layers. Since A is very sparse, we only show entries with scores larger than
0.1. These results use FoX (Pro). More examples can be found in Appendix F.10.
training context length. Similar to the Transformer, it maintains a monotonically decreasing per-
token loss within the training context length, indicating that it utilizes the entire training context for
its prediction. In contrast, the per-token loss curves of all recurrent sequence models start flattening
at around 5k tokens and plateau after 10k tokens. This indicates that these recurrent models struggle
to use the full context effectively for their prediction. In terms of the absolute values of the loss and
perplexity, FoX (Pro) also clearly outperforms HGRN2, DeltaNet and Mamba-2.
Visualization of forget gate values and attention map In Figure 3, we visualize the forget gate
weight matrix F and the attention scores A = softmax(QK⊤ +log F) from two heads in different
layers. The head on the left-hand side exhibits strong decay, and most entries ofF are close to zero;
accordingly, the attention focuses on local entries. The head on the right-hand side has much weaker
decay, and the attention is distributed across the entire context. This shows that FoX can learn to
retain information across long contexts when necessary.
4.3 N EEDLE IN THE HAYSTACK
The needle-in-the-haystack test (Kamradt, 2023) is a popular test for the long-context retrieval abil-
ities of language models. Besides the standard mode where the “needle” only includes the answer to
be retrieved, we also use an “easy mode” (Qin et al., 2024a) where the “needle” placed in the context
includes both the question and the answer. This easy mode is particularly suitable for base models
that have not been instruction-tuned. Full details, including the prompts used, are in Appendix B.3.
In Figure 4, we show the results for different models. HGRN2 performs even worse than Mamba-2
and we leave it to Appendix F.5. As shown in Figure 4, FoX achieves near-perfect needle retrieval
within the training context length in all cases. Transformer (Pro) and Transformer (LLaMA) also
have perfect accuracy within the training context length in the easy mode, though they sometimes
fail in the standard mode.3 In contrast, Mamba-2 and DeltaNet (and also HGRN2 in Appendix F.5)
perform poorly even within the training context length. FoX (Pro), FoX (LLaMA), and Transformer
(Pro) also partially extrapolate beyond the training context length. This is expected given their
per-token loss pattern beyond the training context length (see Figure 2). However, we find that
the extrapolation behaviors of these models could be hyperparameter-dependent. For example, in
Figure 5, we show that for FoX (Pro), the needle retrieval results and the per-token loss slope beyond
the training context length vary depending on the number of training tokens and learning rates. In
particular, we find that more training tokens often leads to worse extrapolation, indicating that the
models may be gradually “overfitting” to their training context length during training.
4.4 D OWNSTREAM TASKS
We use two sets of downstream tasks: a set of short-context tasks from LM-evaluation-harness (Gao
et al., 2024) and a set of long-context tasks from LongBench (Bai et al., 2023).
3Note these are small models without instruction-tuning. We expect that with more parameters/training
tokens or instruction-tuning Transformers should also have perfect accuracy within the training context length.
6
Published as a conference paper at ICLR 2025
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (LLaMA), Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (LLaMA), Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Mamba-2, Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
DeltaNet, Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), Standard
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (LLaMA), Standard
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), Standard
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (LLaMA), Standard
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Mamba-2, Standard
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
DeltaNet, Standard
2 4 6 8 10
Score
Figure 4: Needle-in-the-haystack analysis for different models. We show results for the easy mode
on the left and the standard mode on the right. The results are scored on a scale of 1 to 10 by GPT-
4o-2024-08-06. The vertical dashed line indicates the training context length.
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
16B tokens, LR=0.001
1000410072001030013400165001960022700258002890032000
Document Length
16B tokens, LR=0.002
1000410072001030013400165001960022700258002890032000
Document Length
48B tokens, LR=0.001
1000410072001030013400165001960022700258002890032000
Document Length
48B tokens, LR=0.002
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.5
1.6
1.7
1.8
1.9Loss L(i)
16B tokens, LR=0.001
16B tokens, LR=0.002
48B tokens, LR=0.001
48B tokens, LR=0.002
Figure 5: FoX (Pro) easy mode needle-in-the-haystack results and per-token loss for different num-
bers of training tokens and learning rates. The vertical dashed line indicates the training context
length. More results can be found in Appendix F.7.
Table 1: Evaluation results on LM-eval-harness. All models have roughly760M non-embedding pa-
rameters and are trained on roughly48B tokens on LongCrawl64. “acc-n” means length-normalized
accuracy. Bold and underlined numbers indicate the best and the second best results, respectively.
Model Wiki. LMB.LMB. PIQA Hella. Wino. ARC-e ARC-c COPA OBQA SciQA BoolQAvgppl↓ ppl↓ acc↑ acc↑ acc-n↑ acc↑ acc↑ acc-n↑ acc↑ acc-n↑ acc↑ acc↑ ↑
FoX (Pro) 23.04 14.9142.75 64.09 38.39 52.33 52.23 26.54 71.00 29.80 85.10 46.57 50.88Transformer (Pro)24.1216.16 41.47 64.04 36.60 49.72 51.98 25.26 62.00 29.20 82.80 60.86 50.39FoX (LLaMA) 26.45 18.2740.17 63.44 35.17 51.78 49.66 25.09 69.00 28.00 81.90 54.04 49.82Transformer (LLaMA)28.14 22.3438.27 63.22 34.20 49.49 47.98 24.49 66.00 29.40 78.90 58.93 49.09Mamba-2 28.20 21.0536.50 63.17 35.86 50.59 49.96 25.60 71.00 31.00 80.90 57.49 50.21HGRN2 30.57 20.1438.60 63.49 34.94 51.78 50.13 25.51 66.00 30.00 75.60 58.41 49.45DeltaNet 29.17 29.1434.27 62.73 33.28 50.28 47.39 24.32 70.00 29.00 74.30 54.37 47.99
7
Published as a conference paper at ICLR 2025
Table 2: Evalution results on LongBench. All models have roughly 760M non-embedding parame-
ters and are trained on roughly 48B tokens on LongCrawl64. Bold and underlined numbers indicate
the best and the second-best results, respectively.
Model
Single-Document QA Multi-Document QA Summarization Few-shot Learning Code
NarrativeQAQasperMFQAHotpotQA2WikiMQAMusiqueGovReportQMSumMultiNewsTRECTriviaQASamSumLCCRepoBench-PFoX (Pro) 13.3818.8828.73 15.27 25.39 6.49 22.71 13.51 12.27 63.5 37.3622.7410.9 9.1Transformer (Pro) 11.4221.5422.89 19.58 22.65 6.09 21.92 10.7 8.11 55.0 40.67 30.6610.79 14.25FoX (LLaMA) 10.47 14.81 24.7113.03 21.58 5.25 20.05 10.97 4.86 61.5 34.48 19.13 7.69 8.12Transformer (LLaMA) 11.11 13.5 21.52 9.42 21.33 4.32 18.53 8.43 10.9951.5 28.41 19.17 8.21 14.06Mamba-2 10.65 11.26 16.98 11.59 16.69 5.0 9.31 11.22 10.89 28.5 15.6 16.19 12.07 15.17HGRN2 8.78 10.94 18.66 7.78 15.29 4.32 6.13 12.19 7.83 16.5 14.46 6.37 18.17 16.62DeltaNet 9.36 9.76 16.49 6.57 15.09 2.76 8.19 12.3 7.62 35.5 17.57 18.42 12.24 3.94
512 1024 2048 4096 8192 16384 32768
Token index i
1.9
2.0
2.1Loss L(i)
350M parameters/7.5B training tokens
Model
FoX (Pro)
Transformer (Pro)
Training context length
4096
8192
16384
32768
512 1024 2048 4096 8192 16384 32768
Token index i
1.65
1.70
1.75
1.80
1.85Loss L(i)
760M parameters/16B training tokens
Model
FoX (Pro)
Transformer (Pro)
Training context length
4096
8192
16384
32768
Figure 6: Per-token loss given different model sizes, numbers of training tokens, and training context
lengths. At each token index i, we report the averaged loss over a window of 101 centered at i. We
only show results within the training context length to reduce visual clutter. See Appendix F.6 for
additional results, including length extrapolation and 125M-parameter model results.
Short-context tasks We use Wikitext (Merity et al., 2016), LAMBADA (Paperno et al., 2016),
PiQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Zellers et al., 2019), ARC-
easy, ARC-challenge (Clark et al., 2018), Copa (Roemmele et al., 2011), SciQA (Auer et al., 2023),
OpenbookQA (Mihaylov et al., 2018), and BoolQA (Clark et al., 2019). Following Yang et al.
(2023), we report perplexity for Wikitext and LAMBADA, length-normalized accuracy for Hel-
laSwag, ARC-challenge, and OpenbookQA, and accuracy for all other tasks (we also report accuracy
for LAMBADA). All results are zero-shot. As shown in Table 1, FoX outperforms the Transformer
with either architecture. FoX (Pro) performs the best among all models.
Long-context tasks We use14 tasks from LongBench: HotpotQA (Yang et al., 2018), 2WikiMul-
tihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), MultiFieldQA-en, NarrativeQA (Koˇcisk`y
et al., 2018), Qasper (Dasigi et al., 2021), GovReport (Huang et al., 2021), QMSum (Zhong et al.,
2021), MultiNews (Fabbri et al., 2019), TriviaQA (Joshi et al., 2017), SAMSum (Gliwa et al., 2019),
TREC (Li & Roth, 2002), LCC (Guo et al., 2023), and RepoBench-P (Liu et al., 2023). We use the
default metrics of LongBench for different tasks, which are either F1, Rough-L, accuracy, or edit
similarity. As shown in Table 2, with either architecture, FoX performs on par with the Transformer
and better than the recurrent sequence models.
4.5 A NALYSES
We present three sets of analyses. First, we study how the advantages of FoX over the Transformer
vary with model size and training context length. Second, we investigate the contribution of each
component in FoX and how RoPE affects performance. Finally, we study the importance of using
a forget gate that is data-dependent. For these experiments, we use either 760M-parameter models
trained on roughly 16B tokens or 360M-parameter models trained on roughly 7.5B tokens. Experi-
mental details can be found in Appendix B.
8
Published as a conference paper at ICLR 2025
Table 3: Ablation experiments for FoX. We use 360M-parameter models trained on 7.5B tokens on
LongCrawl64. The perplexity is measured over a validation context length of16384 tokens. For the
bottom half, all addition (+) or removal (-) of components are relative to FoX (Pro).
Model RoPE Forget gate QK-norm Output gate Output norm KV-shiftPerplexity
Transformer (LLaMA) w/o RoPE 29.30Transformer (LLaMA) ✓ 7.49✓ ✓ 7.19FoX (LLaMA) ✓ 7.25✓ ✓ 7.08✓ ✓ ✓ 6.88✓ ✓ ✓ ✓ 6.80FoX (Pro) ✓ ✓ ✓ ✓ ✓ 6.62
- QK-norm ✓ ✓ ✓ ✓ 6.79- output gate ✓ ✓ ✓ ✓ 6.86- output norm ✓ ✓ ✓ ✓ 6.69- KV-shift ✓ ✓ ✓ ✓ 6.80+ RoPE ✓ ✓ ✓ ✓ ✓ ✓ 6.63- forget gate + RoPE (i.e. Transformer (Pro))✓ ✓ ✓ ✓ ✓ 6.82- forget gate ✓ ✓ ✓ ✓ 7.40
0 20000 40000 60000
Token index i
1.9
2.0
2.1
2.2Loss L(i)
FoX (LLaMA), data-dep
fixed, Tmax = 256
fixed, Tmax = 2048
fixed, Tmax = 16384
0 20000 40000 60000
Token index i
1.9
2.0
2.1
2.2Loss L(i)
FoX (LLaMA), data-dep
data-indep, Tmax = 256
data-indep, Tmax = 2048
data-indep, Tmax = 16384
0 20000 40000 60000
Token index i
1.8
1.9
2.0
2.1
2.2Loss L(i)
FoX (Pro), data-dep
fixed, Tmax = 256
fixed, Tmax = 2048
fixed, Tmax = 16384
0 20000 40000 60000
Token index i
1.8
1.9
2.0
2.1
2.2Loss L(i)
FoX (Pro), data-dep
data-indep, Tmax = 256
data-indep, Tmax = 2048
data-indep, Tmax = 16384
Figure 7: Data-dependent forget gate (data-dep) vs. data-independent (data-indep) and fixed forget
gate. ( left and middle-left) Comparison using the LLaMA architecture. ( middle-right and right)
Comparison using the Pro architecture. We use 360M-parameter models trained on roughly 7.5B
tokens on LongCrawl64. All per-token loss curves are smoothed with a moving average sliding
window of 1001 tokens. The vertical dashed line indicates the training context length.
Model size and training context length In Figure 6, we show the per-token loss for two different
model sizes (trained on different numbers of tokens) and several training context lengths for FoX
(Pro) and Transformer (Pro). As shown in Figure 6, the advantages of FoX over Transformer (1)
increase as we increase the training context length and (2) decrease as we increase the model size
(and training tokens). This indicates that the advantages of having a forget gate might depend on the
ratio between the model size and the training context length, as larger models can better model long
contexts, and thus forgetting may be less important. We also note that long-context training damages
short-context performance, which is a known effect (Ren et al., 2024; Sun et al., 2024) likely due to
reduced document diversity within training batches.
Component analysis We present both (1) an “incremental” style analysis where we incrementally
add/remove components from Transformer (LLaMA) to obtain the complete FoX (Pro) model and
(2) a “perturbation” style analysis where we add/remove components from FoX (Pro). The results
are shown in Table 3. First, as mentioned previously, adding RoPE to FoX (LLaMA) and FoX (Pro)
results in minor and no improvement, respectively. Second, both types of analyses show that all
components in FoX contribute positively. Also note that models that use neither forget gates nor
RoPE perform poorly (the first and the last row of the table).
Data-independent and fixed forget gates To show the importance of using a forget gate that is
data-dependent, we test a data- independent forget gate f(h)
t = σ(b(h)), where the superscript (h)
means for the h-th head. We also test a forget gate that has fixed values (i.e., f(h)
t = σ(b(h)),
but we do not update b(h)). As mentioned in Section 3, using a fixed forget gate is equivalent to
ALiBi. For these data-independent forget gate designs, we find it crucial to initialize b(h) properly.
In particular, we intialize {b(h)}H
h=1 for the H heads with two hyperparameter Tmin and Tmax such
9
Published as a conference paper at ICLR 2025
that using fixed forget gates with (Tmin, Tmax) is equivalent to ALiBi with a minimum slope 1
Tmax
and a maximum slope 1
Tmin
. This initialization method is detailed in Appendix D.
In Figure 7, we show the per-token loss of different forget gate designs with the LLaMA and the Pro
architecture. For the data-independent and the fixed forget gate designs, we set Tmin = 2and test
different values of Tmax. As shown in Figure 7, a data-dependent forget gate always has the best
performance.
5 R ELATED WORK
Recurrent sequence models There has been a growing interest in reviving recurrent sequence
models (Katharopoulos et al., 2020; Peng et al., 2021; Gu et al., 2021; Orvieto et al., 2023; Yang
et al., 2023; Gu & Dao, 2023; Katsch, 2023; De et al., 2024; Sun et al., 2024; Peng et al., 2024; Qin
et al., 2024a; Dao & Gu, 2024; Beck et al., 2024; Zhang et al., 2024; Buckman et al., 2024). Many
recent recurrent sequence models feature some form of the forget gate, which has been shown to
be essential in these architectures (Qin et al., 2024b; Gu & Dao, 2023; Yang et al., 2023). Notably,
GLA (Yang et al., 2023) and Mamba-2 (Dao & Gu, 2024) show that gated variants of linear attention
could be written in a form similar to softmax attention, which directly inspired our work. Several
works (Ma et al., 2022; 2024; Ren et al., 2024) combine recurrent layers with quadratic attention.
However, unlike our method – which embeds the forget gate into the attention mechanism – in these
hybrid architectures, the recurrent layers and the quadratic attention layers are independent.
Related improvements and alternatives to softmax attention Several positional embedding
methods (Press et al., 2021; Raffel et al., 2020; Chi et al., 2022a;b) add bias terms to the attention
logits based on the distances between the keys and queries, which can implement data-independent
decay. LAS-attention (Zimerman & Wolf) applies multiplicative exponential decay to the attention
logits. RoPE (Su et al., 2024) also has a similar decay effect that becomes stronger with increas-
ing relative query/key distances. However, all these methods can only achieve data- independent
decay based on the relative distances between the queries and keys. CoPE (Olsson et al., 2022)
and Selective Attention (Leviathan et al., 2024) modify the current timestep’s attention logit based
on the sum of transformed logits from some previous timesteps. Our method differs from these in
various aspects. For example, in our approach, there is no need to compute sums of transformed
logits, which may come with several issues such as potential incompatibility with FlashAttention.
Geometric attention (Csord´as et al., 2021) and stick-breaking attention (Tan et al., 2024) use a stick-
breaking process to compute the attention scores, which has a similar data-dependent decay effect
to our method. These methods explore in a different direction from ours, as they seek to develop
alternatives to softmax attention while our approach is only an improvement on softmax attention.
6 C ONCLUSION
We propose the Forgetting Transformer (FoX), a Transformer variant with a forget gate. Our ex-
periments show that FoX outperforms the Transformer and several recurrent sequence models on
long-context language modeling and various downstream tasks. We also show that our Pro block
design greatly outperforms the basic LLaMA architecture, with or without a forget gate. We there-
fore recommend that future work adopt FoX (Pro) and Transformer (Pro) as baselines in addition to
the commonly used LLaMA architecture.
We discuss several limitations of our work and potential future work. First, due to our limited
computing resources, our main experiments only use models up to 760M parameters, 48B tokens,
and a training context length of 16384 tokens. Thus, an important direction for future work is to test
FoX at larger scales. Second, we only consider causal sequence modeling in this work. It would
be interesting to extend Forgetting Attention to the non-causal case. Finally, we could potentially
prune computation (e.g., KV-cache eviction) adaptively based on the forget gate values, which may
greatly reduce training and inference costs.
10
Published as a conference paper at ICLR 2025
ACKNOWLEDGMENTS
ZL thanks Songlin Yang, Jacob Buckman, Zhen Qin, Artem Zholus, Benjamin Th ´erien, Jonathan
Pilault, and Mahan Fathi for their helpful discussion. AC acknowledges funding from Microsoft
research. This research was enabled in part by the compute resources, software and technical help
provided by Mila ( mila.quebec) and the Digital Research Alliance of Canada ( alliance.
can.ca).
REFERENCES
S¨oren Auer, Dante AC Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver
Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, et al. The
sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13
(1):7240, 2023.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long
context understanding. arXiv preprint arXiv:2308.14508, 2023.
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi
Li. Longalign: A recipe for long context alignment of large language models. arXiv preprint
arXiv:2401.18058, 2024.
Maximilian Beck, Korbinian P ¨oppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova,
Michael Kopp, G¨unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended
long short-term memory. arXiv preprint arXiv:2405.04517, 2024.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-
monsense in natural language. In Proceedings of the AAAI conference on artificial intelligence ,
volume 34, pp. 7432–7439, 2020.
Jacob Buckman. Longcrawl64: A Long-Context Natural-Language Dataset, 2024. URL https:
//manifestai.com/articles/longcrawl64.
Jacob Buckman, Carles Gelada, and Sean Zhang. Symmetric Power Transformers, 2024.
Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple: Kernelized rel-
ative positional embedding for length extrapolation. Advances in Neural Information Processing
Systems, 35:8386–8399, 2022a.
Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky, and Peter J Ramadge. Dissecting transformer
length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356 ,
2022b.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint
arXiv:1905.10044, 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457, 2018.
R´obert Csord ´as, Kazuki Irie, and J ¨urgen Schmidhuber. The neural data router: Adaptive control
flow in transformers improves systematic generalization.arXiv preprint arXiv:2110.07732, 2021.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691, 2023.
Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality. arXiv preprint arXiv:2405.21060, 2024.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset
of information-seeking questions and answers anchored in research papers. arXiv preprint
arXiv:2105.03011, 2021.
11
Published as a conference paper at ICLR 2025
Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Al-
bert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mix-
ing gated linear recurrences with local attention for efficient language models. arXiv preprint
arXiv:2402.19427, 2024.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling
vision transformers to 22 billion parameters. In International Conference on Machine Learning,
pp. 7480–7512. PMLR, 2023.
Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R Radev. Multi-news: A large-
scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint
arXiv:1906.01749, 2019.
FlagOpen, 2023. URL https://github.com/FlagOpen/FlagAttention.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-
ter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-
nighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-
tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/
12608602.
Felix A Gers, J ¨urgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with lstm. Neural computation, 12(10):2451–2471, 2000.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-
annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237 ,
2019.
Klaus Greff, Rupesh K Srivastava, Jan Koutn´ık, Bas R Steunebrink, and J¨urgen Schmidhuber. Lstm:
A search space odyssey. IEEE transactions on neural networks and learning systems , 28(10):
2222–2232, 2016.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752, 2023.
Albert Gu, Karan Goel, and Christopher R ´e. Efficiently modeling long sequences with structured
state spaces. arXiv preprint arXiv:2111.00396, 2021.
Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range pre-
trained language model for code completion. In International Conference on Machine Learning,
pp. 12098–12107. PMLR, 2023.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop
qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060 ,
2020.
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and
Boris Ginsburg. Ruler: What’s the real context size of your long-context language models?arXiv
preprint arXiv:2404.06654, 2024.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for
long document summarization. arXiv preprint arXiv:2104.02112, 2021.
Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Trans-
formers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.
Gregory Kamradt, 2023. URL https://github.com/gkamradt/LLMTest_
NeedleInAHaystack/blob/main/README.md.
12
Published as a conference paper at ICLR 2025
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on ma-
chine learning, pp. 5156–5165. PMLR, 2020.
Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv
preprint arXiv:2311.01927, 2023.
Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,
and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the
Association for Computational Linguistics, 6:317–328, 2018.
Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer.arXiv
preprint arXiv:2410.02703, 2024.
Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics, 2002.
Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code
auto-completion systems. arXiv preprint arXiv:2306.03091, 2023.
I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint
arXiv:2209.10655, 2022.
Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke
Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference
with unlimited context length. arXiv preprint arXiv:2404.08801, 2024.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,
2018.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
OpenAI, 2021. URL https://github.com/triton-lang/triton.
OpenAI, 2022. URL https://github.com/openai/tiktoken.
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pas-
canu, and Soham De. Resurrecting recurrent neural networks for long sequences. InInternational
Conference on Machine Learning, pp. 26670–26698. PMLR, 2023.
Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The lambada dataset:
Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.
Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene
Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with
matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.
Random feature attention. arXiv preprint arXiv:2103.02143, 2021.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
13
Published as a conference paper at ICLR 2025
Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong.
Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a.
Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for se-
quence modeling. Advances in Neural Information Processing Systems, 36, 2024b.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of machine learning research, 21(140):1–67, 2020.
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Sim-
ple hybrid state space models for efficient unlimited context language modeling. arXiv preprint
arXiv:2406.07522, 2024.
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives:
An evaluation of commonsense causal reasoning. In 2011 AAAI spring symposium series, 2011.
Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for
linear complexity language models. arXiv preprint arXiv:2406.16690, 2024.
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hes-
tness, and Nolan Dey. SlimPajama: A 627B token cleaned and dedu-
plicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama ,
2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
hanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei
Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive
hidden states. arXiv preprint arXiv:2407.04620, 2024.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and
Furu Wei. Retentive network: A successor to transformer for large language models. arXiv
preprint arXiv:2307.08621, 2023.
Shawn Tan, Yikang Shen, Songlin Yang, Aaron Courville, and Rameswar Panda. Stick-breaking
attention. arXiv preprint arXiv:2410.17980, 2024.
Together Computer. Redpajama: an open dataset for training large language models, 2023. URL
https://github.com/togethercomputer/RedPajama-Data.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop
questions via single-hop question composition.Transactions of the Association for Computational
Linguistics, 10:539–554, 2022.
Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate.arXiv
preprint arXiv:1804.04849, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.,
2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
14
Published as a conference paper at ICLR 2025
Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert
Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-
based language models. arXiv preprint arXiv:2406.07887, 2024.
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin,
Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling
of foundation models. arXiv preprint arXiv:2309.16039, 2023.
Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of
linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/
flash-linear-attention.
Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention
transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.
Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transform-
ers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. arXiv preprint arXiv:1809.09600, 2018.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-
chine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda
Shi, Bailin Wang, Wei Bi, et al. Gated slot attention for efficient linear-time sequence modeling.
arXiv preprint arXiv:2409.07146, 2024.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadal-
lah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based
multi-domain meeting summarization. arXiv preprint arXiv:2104.05938, 2021.
Itamar Zimerman and Lior Wolf. Viewing transformers through the lens of long convolutions layers.
In Forty-first International Conference on Machine Learning.
15
Published as a conference paper at ICLR 2025
Appendix
Table of Contents
A Detailed FoX (Pro) layer computation 17
B Experimental details 17
B.1 Model and training hyperparameters . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 Model parameters, estimated FLOPs, and throughput . . . . . . . . . . . . . . . 18
B.3 Needle in the haystack details . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Explanation on the relationship between per-token-loss slope and context utilization 19
D Data-independent forget gate initialization 19
E Hardware-aware implementation of Forgetting Attention 20
F Additional results 22
F.1 Per-token loss for the ablation studies . . . . . . . . . . . . . . . . . . . . . . . 22
F.2 Transformer (Pro) ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
F.3 Short-context training on SlimPajama . . . . . . . . . . . . . . . . . . . . . . . 22
F.4 Additional comparison with sliding window attention and Samba . . . . . . . . 25
F.5 Additional needle-in-the-haystack results . . . . . . . . . . . . . . . . . . . . . 25
F.6 Additional results with 125M-param/2.7B-token, 360M-param/7.5B-token, and
760M-param/16B-token training configurations . . . . . . . . . . . . . . . . . . 27
F.7 Sensitivity of length extrapolation behaviors to hyperparameters . . . . . . . . . 27
F.8 Training curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
F.9 Stability across random seeds . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
F.10 Additional visualization of forget gate and attention score matrices . . . . . . . . 33
16
Published as a conference paper at ICLR 2025
A D ETAILED FOX (P RO) LAYER COMPUTATION
We describe the computation of a FoX (Pro) layer (depicted in Figure 1 right) in detail. Each FoX
layer takes an input sequence (xi)L
i=1 ∈ Rd and produces an output sequence (yi)L
i=1 ∈ Rd. We
first describe the computation for the single head case with head dimensiondhead. For each time step
t, we first compute the key kt ∈ Rdhead , query qt ∈ Rdhead , value vt ∈ Rdhead , forget gate ft ∈ R, and
output gate gt ∈ Rdhead as follows:
qt = RMSNorm(Wqxt) (15)
˜kt = Wkxt, α key
t = σ(w⊤
k xt) ∈ R (16)
kt = RMSNorm(αkey
t ˜kt−1 + (1− αkey
t )˜kt) (17)
˜vt = Wvxt, α value
t = σ(w⊤
v xt) ∈ R (18)
vt = αvalue
t ˜vt−1 + (1− αvalue
t )˜vt (19)
ft = σ(w⊤
f xt + bf ) ∈ R (20)
gt = σ(Wgxt). (21)
This is followed by the computation of the attention output:
oi =
Pi
j=1 exp(q⊤
i kj + Dij)vj
Pi
j=1 exp(q⊤
i kj + Dij)
∈ Rdhead (22)
where Dij = Pi
l=j+1 log fl with Dii = 0for any i. We then apply the output normalization, output
gate, and the final output projection:
yi = Wo(RMSNorm(oi) ⊙ gi) ∈ Rd. (23)
For the multi-head case, each head h maintains an independent copy of the parameters and com-
putes its output sequence (y(h)
i )L
i=1 independently. All RMSNorms are also applied independently
to each head.4 y(h)
i ’s from different heads are then summed together to obtain the final output yi.
Note that similar to the standard Transformer, even though the computation and parameters of dif-
ferent heads are conceptually independent, most computations can be implemented equivalently by
properly splitting/concatenating the intermediate vectors/weight matrices of different heads.
B E XPERIMENTAL DETAILS
B.1 M ODEL AND TRAINING HYPERPARAMETERS
Configuration nlayers dmodel dhead Peak learning rate
760M params / 48B tokens 24 1536 64 for FoX, 128 for Transformer See Table 5
760M params / 16B tokens 24 1536 64 for FoX, 128 for Transformer 1 ×10−3
360M params / 7.5B tokens 24 1024 64 2 ×10−3
125M params / 2.7B tokens 12 768 64 2 ×10−3
Table 4: Hyperparameters for different configurations. The head dimension dhead is only applicable
to FoX and the Transformer. We tunedhead for the 760M-parameter FoX and Transformer models in
{64, 128}. nlayer counts the number of blocks. For example, for the Transformer each block contains
an attention layer and an MLP layer.
We list the hyperparameters for different training configurations used in this work in Table 4. For
FoX and Transformer, we follow the HuggingFace LLaMA initialization and initialize all linear
layer weights and embedding parameters with N(0, 0.022). Other hyperparameters are either men-
tioned in the main text (Section 4.1) or follow the default values in the Flash Lienar Attention repos-
itory (Yang & Zhang, 2024)5. Note that our main experiments use the 760M-parameter/48B-token
4For the QK-norm implementation, we accidentally shared a single set of dhead RMSNorm scaling pa-
rameters across different heads in each layer for our experiments (normally there should be one set of dhead
parameters for each head). We have verified that this has no observable impact on performance.
5Commit 1c5937eeeb8b0aa17bed5ee6dae345b353196bd4.
17
Published as a conference paper at ICLR 2025
Model Learning rate
FoX (Pro) 2 × 10−3
Transformer (Pro) 1 × 10−3
FoX (LLaMA) 1 × 10−3
Transformer (LLaMA) 5 × 10−4
Mamba-2 2 × 10−3
HGRN2 2 × 10−3
DeltaNet 1 × 10−3
Table 5: Peak learning rates for different models for the 760M-parameter/48B-token main experi-
ments. These are tuned using a grid {1 × 10i, 2 × 10i, 5 × 10i} with different values of i.
configuration. The other three configurations are for ablation studies and additional analyses. For
the 760M-parameter/48B-token experiments, we tune the learning rate for each model. We also tune
the head dimension for each Transformer and FoX model, along with learning rates. For the other
three configurations used for ablation studies and additional analyses, the learning rates are tuned
for Transformer (LLaMA) for the 16k training context length setting and transferred to other models
and training context lengths. The head dimensions for these three settings are tuned for Transformer
(LLaMA) and FoX (LLaMA) for the 16k training context length setting and transferred to Trans-
former (Pro) and FoX (Pro) and other training context lengths. The optimal hyperparameters are
chosen based on the average training loss over the last 512 × 220 (or 512M) tokens. Note that
each token is only visited once by the models during training, so there is no fundamental difference
between the training and validation loss.
We do not share the parameters between the embedding layer and the output layer. Following the
original LLaMA architecture, no bias terms are used in linear layers, except for forget gates.6 Weight
decay is not applied to RMSNorm parameters and bias terms in linear layers (again, only used for
forget gates) or convolution layers. We use bfloat16 mixed-precision training for all models.
B.2 M ODEL PARAMETERS , ESTIMATED FLOP S, AND THROUGHPUT
We report the number of (non-embedding) parameters, estimated FLOPs, and training throughput
in Table 6. For the recurrent models, we estimate FLOPs using their recurrent form for simplic-
ity. Note that the FlashAttention kernels for FoX (Pro), Transformer (Pro), and FoX (LLaMA)
are implemented in Triton by us on top of Flag Attention (FlagOpen, 2023) without significant
optimization, while Transformer (LLaMA) uses the official FlashAttention implementation (Dao,
2023)) in CUDA. Also, we did not focus on optimizing the efficiency of components such as QK-
norm and KV-shift. We expect these four models to have similar throughput if they are properly
optimized. Since we use a long training context length compared to model size, recurrent models
have a significant advantage in theoretical FLOPs due to their linear complexity. Though an exact
FLOPs-matched comparison would be interesting, it will require recalibrating the scaling law for
the long-context setting and is beyond the scope of this work.
Model Params Forward FLOPs/token Formula for FLOPs/token Throughput (tokens/sec)
FoX (Pro) 759M 2.72×109 2N+ 2nlayerdmodelL 27k
Transformer (Pro) 757M 2.72×109 2N+ 2nlayerdmodelL 30k
FoX (LLaMA) 757M 2.72×109 2N+ 2nlayerdmodelL 30k
Transformer (LLaMA)756M 2.72×109 2N+ 2nlayerdmodelL 38k
Mamba-2 780M 1.65×109 2N+ 20nlayerdmodeldstate 44k
HGRN2 756M 1.54×109 2N+ 5nlayerdmodeldhead 46k
DeltaNet 757M 1.54×109 2N+ 6nlayerdmodeldhead 48k
Table 6: Number of parameters, estimated forward pass FLOPs per token, formulas for FLOPs esti-
mation, and training throughput in tokens per second for different models. Throughput is measured
on 4 NVIDIA L40S GPUs. N is the number of parameters and L is the training context length.
6In preliminary small-scale experiments, we do not find bias terms in forget gates to matter for performance
in a statistically significant way. We keep it as it might be useful in some cases.
18
Published as a conference paper at ICLR 2025
B.3 N EEDLE IN THE HAYSTACK DETAILS
Our needle-in-the-haystack test is based on LongAlign (Bai et al., 2024), which is adapted from
the original needle test repositoty (Kamradt, 2023) for HuggingFace 7 models. The prompt for the
standard mode has the following structure:
[irrelevant context...]
The best thing to do in San Francisco is eat a sandwich and sit in
Dolores Park on a sunny day.
[irrelevant context...]
There is an important piece of information hidden inside the above
document. Now that you’ve read the document, I will quiz you about it.
Answer the following question: What is the best thing to do in San
Francisco? Answer: The best thing to do in San Francisco is
The easy mode is the same, except the needle placed within the context also includes the question:
[irrelevant context...]
What is the best thing to do in San Francisco? Answer: The best thing to
do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny
day.
[irrelevant context...]
There is an important piece of information hidden inside the above
document. Now that you’ve read the document, I will quiz you about it.
Answer the following question: What is the best thing to do in San
Francisco? Answer: The best thing to do in San Francisco is
The results are scored by GPT-4o-2024-08-06 on a scale from 1 to 10.
C E XPLANATION ON THE RELATIONSHIP BETWEEN PER -TOKEN -LOSS SLOPE
AND CONTEXT UTILIZATION
To understand the relationship between the slope of the per-token loss and context utilization of
the model, we first point out that LongCrawl64 applies the preprocessing of randomly “rolling” the
sequences8 to remove any position bias. This means that when given contexts of the same length ,
the difficulty of predicting tokens at different positions is roughly the same in expectation. 9 For
example, in expectation, predicting the 100-th token in a sequence given only the previous90 tokens
as the context is roughly as difficult as predicting the 90-th token given the full previous 90-token
context. Therefore, if L(100) < L(90), it indicates that the first 10 tokens in the context contribute
to the model’s predictions for the 100-th token; and larger the difference L(90) − L(100) is, the
more these distant tokens contribute. On the other hand, if L(100) is roughly the same L(90) (i.e.,
the graph of L(i) plateaus after i = 100), it means the first10 tokens do not contribute to the model’s
prediction for the 100-th token, either because they are inherently not useful for this prediction or
the model are unable to utilize them.
In summary, the slope of L(i) at token position i reflects how much tokens from roughly i steps
earlier contribute to the model’s prediction at the current token position.
D D ATA-INDEPENDENT FORGET GATE INITIALIZATION
To understand our initialization for the fixed forget gate and the data-independent forget gate, we first
define a function T(b) = 1
−log σ(b) . This function is defined such that σ(b)T(b) = 1/e is always true
7https://huggingface.co/
8Concretely, this can be implemented with np.roll with random shift value.
9Note that even without random rolling, given the same number of previous tokens as the context, the
difficulty of token prediction at different positions may still remain relatively uniform.
19
Published as a conference paper at ICLR 2025
(i.e., T(b) is the timesteps needed to achieve a decay of1/e). We then initializeb(h) = b(h)
init such that
T(b(h)
(init)) = exp(logTmin +(log Tmax −log Tmin) h−1
H−1 ), where Tmin and Tmax are hyperparameters
and H is the number of heads. For example, if h = 4and (Tmin, Tmax) = (2, 128), then we have
(T(b(1)
init ), T(b(2)
init ), T(b(3)
init ), T(b(4)
init )) = (2, 8, 32, 128). As mentioned in the main text, A fixed forget
gate with (Tmin, Tmax) is equivalent to ALiBi with a minimum slope 1
Tmax
and a maximum slope
1
Tmin
. We also tested this initialization for the data-dependent forget gate but did not find it useful, so
we simply initialize {b(h)}H
h=1 to zero for the data-dependent forget gate. For the data-independent
and fixed forget gate, zero initialization performs extremely poorly.
E H ARDWARE -AWARE IMPLEMENTATION OF FORGETTING ATTENTION
Algorithm 1 Forgetting Attention Forward Pass
Require: Matrices Q, K, V ∈ RN×d, vector c ∈ RN in HBM, block sizes Bc, Br.
1: Divide Q into Tr =
l
N
Br
m
blocks Q1, . . . ,QTr of size Br × d each, and divide K, V in to
Tc =
l
N
Bc
m
blocks K1, . . . ,KTc and V1, . . . ,VTc , of size Bc × d each.
2: Divide the output O ∈ RN×d into Tr blocks O1, . . . ,OTr of size Br × d each, and divide the
logsumexp L into Tr blocks L1, . . . , LTr of size Br each.
3: Let cq = c. Devide cq into Tr blocks cq
1, . . . ,cq
Tr
4: Let ck = c. Devide ck into Tc blocks ck
1, . . . ,ck
Tc
5: for 1 ≤ i ≤ Tr do
6: Load Qi, cq
i from HBM to on-chip SRAM.
7: On chip, initialize O(0)
i = (0)Br×d ∈ RBr×d, ℓ(0)
i = (0)Br ∈ RBr , m(0)
i = (−∞)Br ∈
RBr .
8: for 1 ≤ j ≤ Tc do
9: Load Kj, Vj, ck
j from HBM to on-chip SRAM.
10: On chip, compute S(j)
i = QiKT
j ∈ RBr×Bc .
11: On chip, compute D(j)
i = cq
i 1⊤ − 1(ck
j )⊤ ∈ RBr×Bc .
12: On chip, compute S(j)
i = S(j)
i + D(j)
i ∈ RBr×Bc .
13: On chip, compute S(j)
i = mask(S(j)
i , i, j) ∈ RBr×Bc .
14: On chip, compute m(j)
i = max(m(j−1)
i , rowmax(S(j)
i )) ∈ RBr , ˜P(j)
i = exp(S(j)
i −
m(j)
i ) ∈ RBr×Bc (pointwise), ℓ(j)
i = em(j−1)
i −m(j)
i ℓ(j−1)
i + rowsum(˜P(j)
i ) ∈ RBr .
15: On chip, compute O(j)
i = diag(em(j−1)
i −m(j)
i )−1O(j−1)
i + ˜P(j)
i Vj.
16: end for
17: On chip, compute Oi = diag(ℓ(Tc)
i )−1O(Tc)
i .
18: On chip, compute Li = m(Tc)
i + log(ℓ(Tc)
i ).
19: Write Oi to HBM as the i-th block of O.
20: Write Li to HBM as the i-th block of L.
21: end for
22: Return the output O and the logsumexp L.
In Algorithm 1 and 2, we provide the algorithms for computing the forward pass and backward
pass of Forgetting Attention in a hardware-aware way. The algorithm is reproduced from the
FlashAttention-2 paper (Dao, 2023), with the changes needed to implement Forgetting Attention
added and highlighted. In this algorithm, we assume that we pre-computed the cumulative sums
c = cumsum(logf). The mask operation properly sets some entries of its first operand to −∞ to
satisfy the causality requirement. Note for the backward pass for ease of presentation we combine
20
Published as a conference paper at ICLR 2025
Algorithm 2 Forgetting Attention Backward Pass
Require: Matrices Q, K, V , O, dO ∈ RN×d in HBM, vector c, dc ∈ RN , vector L ∈ RN in
HBM, block sizes Bc, Br.
1: Divide Q into Tr =
l
N
Br
m
blocks Q1, . . . ,QTr of size Br × d each, and divide K, V in to
Tc =
l
N
Bc
m
blocks K1, . . . ,KTc and V1, . . . ,VTc , of size Bc × d each.
2: Divide O into Tr blocks O1, . . . ,OTr of size Br × d each, divide dO into Tr blocks
dO1, . . . ,dOTr of size Br ×d each, and divide L into Tr blocks Li, . . . , LTr of size Br each.
3: Initialize dQ = (0)N×d in HBM and divide it into Tr blocks dQ1, . . . ,dQTr of size Br × d
each. Divide dK, dV ∈ RN×d in to Tc blocks dK1, . . . ,dKTc and dV 1, . . . ,dV Tc , of size
Bc × d each.
4: Let cq = ck = c. Devide cq into Tr blocks cq
1, . . . ,cq
Tr . Devide ck into Tc blocks ck
1, . . . ,ck
Tc .
5: Let dcq = dck = (0)N . Devide dcq into Tr blocks dcq
1, . . . ,dcq
Tr . Devide dck into Tc blocks
dck
1, . . . ,dck
Tc .
6: Compute D = rowsum(dO ◦ O) ∈ Rd (pointwise multiply), write D to HBM and divide it
into Tr blocks D1, . . . , DTr of size Br each.
7: for 1 ≤ j ≤ Tc do
8: Load Kj, Vj, ck
j from HBM to on-chip SRAM.
9: Initialize dKj = (0)Bc×d, dV j = (0)Bc×d on SRAM.
10: for 1 ≤ i ≤ Tr do
11: Load Qi, Oi, dOi, dQi, Li, Di, cq
i from HBM to on-chip SRAM.
12: On chip, compute S(j)
i = QiKT
j ∈ RBr×Bc .
13: On chip, compute D(j)
i = cq
i 1⊤ − 1(ck
j )⊤ ∈ RBr×Bc .
14: On chip, compute S(j)
i = S(j)
i + D(j)
i ∈ RBr×Bc .
15: On chip, compute S(j)
i = mask(S(j)
i , i, j) ∈ RBr×Bc .
16: On chip, compute P(j)
i = exp(Sij − Li) ∈ RBr×Bc .
17: On chip, compute dV j ← dV j + (P(j)
i )⊤dOi ∈ RBc×d.
18: On chip, compute dP(j)
i = dOiV ⊤
j ∈ RBr×Bc .
19: On chip, compute dS(j)
i = P(j)
i ◦ (dP(j)
i − Di) ∈ RBr×Bc .
20: Load dQi from HBM to SRAM, then on chip, updatedQi ← dQi + dS(j)
i Kj ∈ RBr×d,
and write back to HBM.
21: Load dcq
i from HBM to SRAM, then on chip, update dcq
i ← dcq
i + dS(j)
i 1 ∈ RBr , and
write back to HBM.
22: On chip, compute dKj ← dKj + dS(j)
i
⊤
Qi ∈ RBc×d.
23: On chip, compute dck
j ← dck
j − dS(j)
i
⊤
1 ∈ RBc .
24: end for
25: Write dKj, dV j, dck
j to HBM.
26: end for
27: Compute dc = dcq + dck.
28: Return dQ, dK, dV , dc .
the computation of dK, dV , dck and the computation of dQ, dcq in a single algorithm, but in
practice these are computed in two different kernels for implementation simplicity.
21
Published as a conference paper at ICLR 2025
0 20000 40000 60000
Token index i
1.8
1.9
2.0
2.1
2.2Loss L(i)
Transformer (LLaMA)
FoX (LLaMA) + RoPE
FoX (LLaMA)
FoX (LLaMA) + QK-norm
FoX (LLaMA) + QK-norm + output gate
FoX (LLaMA) + QK-norm + output gate + output norm
FoX (LLaMA) + QK-norm + output gate + output norm + KV-shift
Figure 8: Per-token loss for the incremental-style ablation studies presented in Section 4.5. All
models are roughly 360M parameters and are trained on roughly7.5B tokens on LongCrawl64. The
vertical line indicates the training context length.
In practice, we implement Forgetting Attention based on the Triton (OpenAI, 2021) FlashAttention
implementation in Flag Attention (FlagOpen, 2023).
F A DDITIONAL RESULTS
F.1 P ER-TOKEN LOSS FOR THE ABLATION STUDIES
In Figure 8 and Figure 9 we show the per-token loss for the ablation studies presented in Table 3
in Section 4.5. Transformer (LLaMA) without RoPE performs extremely poorly and we show it
separately in Figure 10.
F.2 T RANSFORMER (PRO) ABLATION
In Figure 11, we present a small-scale ablation study using Transformer (Pro) in the 125M-
parameter/2.7B-token setting. We start with Transformer (LLaMA) and add one component at a
time. Notably, we find that QK-norm seems to be helpful for length extrapolation.
F.3 S HORT-CONTEXT TRAINING ON SLIM PAJAMA
To complement our main results in which we perform long-context training on LongCrawl64, we
have also run short-context training on the more commonly used SlimPajama dataset (Soboleva
et al., 2023). We follow the 340M-parameter/15B-token/2k-context-length setting in Yang et al.
(2024). We also use the same hyperparameters and tokenizer as Yang et al. (2024). We train FoX
and Transformer with both the LLaMA and the Pro architecture. We also test Mamba-2, the strongest
recurrent sequence model in our main results.
We show the per-token loss of tested models in Figure 12 and downstream task evaluation results in
Table 7. We use the same set of tasks as Yang et al. (2024) so our results can be directly compared
to those of Yang et al. (2024). As shown in the results, in this short-context training setting FoX
(LLaMA) does not have an advantage over the Transformer (LLaMA) except for length extrapo-
lation, while FoX (Pro) still outperforms Transformer (Pro) in language modeling loss and down-
stream tasks. Note that these are small-scale experiments without extensive hyperparameter tuning
(e.g., learning rate), so the results might not transfer to larger scales with proper hyperparameter
tuning for each model.
22
Published as a conference paper at ICLR 2025
0 20000 40000 60000
Token index i
1.8
1.9
2.0
2.1
2.2Loss L(i)
FoX (Pro)
FoX (Pro) - QK-norm
FoX (Pro) - output gate
FoX (Pro) - output norm
FoX (Pro) - KV-shift
FoX (Pro) + RoPE
FoX (Pro) - forget gate + RoPE
FoX (Pro) - forget gate
Figure 9: Per-token loss for the perturbation-style ablation studies presented in Section 4.5. All
models are roughly 360M parameters and are trained on roughly7.5B tokens on LongCrawl64. The
vertical line indicates the training context length.
0 20000 40000 60000
Token index i
2
3
4
5
6Loss L(i)
Transformer (LLaMA) Transformr (LLaMA) w/o RoPE
Figure 10: Removing RoPE from Transformer (LLaMA) results in poor performance. All models
are roughly 360M parameters and are trained on roughly7.5B tokens on LongCrawl64. The vertical
line indicates the training context length.
0 20000 40000 60000
Token index i
2.3
2.4
2.5
2.6Loss L(i)
Transformer (LLaMA) + QK-norm
Transformer (LLaMA) + QK-norm + output gate
Transformer (LLaMA) + QK-norm + output gate + output norm
Transformer (LLaMA) + QK-norm + output gate + output norm + KV-shift
Transformer (LLaMA)
Figure 11: Incremental style ablation study for Transformer (Pro). All models are roughly 125M
parameters and are trained on roughly 2.7B tokens on LongCrawl64. The vertical line indicates the
training context length.
23
Published as a conference paper at ICLR 2025
0 1k 2k 3k 4k 5k 6k 7k 8k
Token index i
2.40
2.45
2.50
2.55
2.60Loss L(i)
0 1k 2k 3k 4k 5k 6k 7k 8k
Validation context length l
11
12
13
14
15Perplexity P(l)
FoX (Pro)
Transformer (Pro)
FoX (LLaMA) Transformer (LLaMA) Mamba-2
Figure 12: Results on SlimPajama with a training context length of 2048 tokens. All models have
roughly 340M non-embedding parameters and are trained on roughly 15B tokens on SlimPajama.
The vertical line indicates the training context length.
Table 7: Evaluation results on LM-eval-harness for models trained on SlimPajama with a training
context length of 2048 tokens. All models have roughly 340M non-embedding parameters and are
trained on roughly 15B tokens on SlimPajama. “acc-n” means length-normalized accuracy. Bold
and underlined numbers indicate the best and the second best results, respectively. Note the results
for Transformer++ and DeltaNet are from Yang et al. (2024). Note that Transformer++ from Yang
et al. (2024) and Transformer (LLaMA) in our work have exactly the same architecture.
Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c Avg
ppl↓ ppl↓ acc↑ acc↑ acc-n↑ acc↑ acc↑ acc-n↑ ↑
Transformer++ (Yang et al., 2024)28.39 42.69 31.00 63.30 34.00 50.40 44.50 24.20 41.23
DeltaNet (Yang et al., 2024) 28.24 37.37 32.10 64.80 34.30 52.20 45.80 23.20 42.07
FoX (Pro) 25.69 31.98 35.82 65.61 36.39 51.07 45.79 25.09 43.29
Transformer (Pro) 25.92 31.93 35.01 65.02 36.09 50.51 46.42 23.38 42.74
FoX (LLaMA) 27.86 43.26 32.56 64.80 34.59 50.12 45.12 23.38 41.76
Transformer (LLaMA) 27.98 35.25 32.31 63.71 34.89 48.07 45.33 23.72 41.34
Mamba-2 27.51 41.32 29.83 65.94 35.95 50.20 45.45 23.72 41.85
24
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.65
1.70
1.75
1.80Loss L(i)
0 8k 16k 24k 32k 40k 48k 56k 64k
Validation context length l
5.25
5.50
5.75
6.00
6.25
6.50Perplexity P(l)
FoX (Pro)
Transformer (Pro)
FoX (LLaMA)
Transformer (LLaMA)
Samba
Transformer-SWA (LLaMA)
Mamba-2
Figure 13: Additional comparison with Samba and Transformer-SW A. (left) Per-token loss L(i) at
different token position i. (right) Validation perplexityP(l) over different validation context length
l. All models have 760M parameters and are trained on roughly 16B tokens. The vertical dashed
line indicates the training context length. The per-token loss is typically noisy, so we smooth the
curve using a moving average sliding window of 101 tokens. In this plot 1k = 1024.
Table 8: Evaluation results on LM-eval-harness. All models have roughly760M non-embedding pa-
rameters and are trained on roughly16B tokens on LongCrawl64. “acc-n” means length-normalized
accuracy. Bold and underlined numbers indicate the best and the second best results, respectively.
Model Wiki. LMB.LMB. PIQA Hella. Wino. ARC-e ARC-c COPA OBQA SciQA BoolQAvgppl↓ ppl↓ acc↑ acc↑ acc-n↑ acc↑ acc↑ acc-n↑ acc↑ acc-n↑ acc↑ acc↑ ↑
FoX (Pro) 28.10 23.6736.93 61.6433.44 49.72 47.94 23.98 65.00 26.80 80.40 57.4948.33Transformer (Pro) 28.1724.6336.1761.5333.46 50.28 47.81 24.15 67.00 28.40 77.90 55.7248.24FoX (LLaMA) 31.03 28.4134.89 61.21 32.27 50.51 46.68 24.0667.00 29.60 77.30 61.0748.46Transformer (LLaMA)32.33 34.4132.41 60.94 31.68 49.96 45.62 23.63 64.00 28.6074.00 60.0647.09Samba 31.71 27.7834.25 60.45 32.8851.70 49.03 24.32 61.00 28.20 78.80 60.5848.12Transformer-SW A (LLaMA)33.63 33.0433.15 60.01 31.83 51.1446.93 23.38 62.00 27.40 76.70 54.62 46.72Mamba-2 33.26 42.3827.29 60.83 32.03 50.67 46.21 23.55 64.00 28.40 76.70 57.6146.73
F.4 A DDITIONAL COMPARISON WITH SLIDING WINDOW ATTENTION AND SAMBA
In this section, we compare the standard Transformer, FoX, and Mamba-2 with a sliding-window-
attention-based Transformer (Transformer-SW A). We also compare with Samba (Ren et al., 2024),
a hybrid architecture combining sliding window attention and Mamba. Both Transformer-SW A and
Samba use a window size of 2048. For these experiments, we use the 760M-parameter/16B-token
configuration in Table 4. Note that as mentioned in Section B, all models in this configuration use
the same learning rate that is tuned for Transformer (LLaMA), so the results might not be optimal
for other models. We show the per-token loss, easy-mode needle-in-the-haystack experiment, short-
context downstream task results, and long-context task results in Figure 13, Figure 14, Table 8, and
Table 9, respectively. Though both Transformer-SW A and Samba perform well on short-context
tasks, they show an early plateau in the per-token loss, which indicates that they struggle to utilize
the long context. Accordingly, they perform poorly in the needle-retrieval task.
F.5 A DDITIONAL NEEDLE -IN-THE -HAYSTACK RESULTS
In Figure 15, we show the results of the needle test for HGRN2 in the 760M-parameter/48B-token
setting.
25
Published as a conference paper at ICLR 2025
Table 9: Evalution results on LongBench. All models have roughly 760M non-embedding parame-
ters and are trained on roughly 16B tokens on LongCrawl64. Bold and underlined numbers indicate
the best and the second best results, respectively.
Model
Single-Document QA Multi-Document QA Summarization Few-shot Learning Code
NarrativeQAQasperMFQAHotpotQA2WikiMQAMusiqueGovReportQMSumMultiNewsTRECTriviaQASamSumLCCRepoBench-PFoX (Pro) 10.4812.98 20.626.87 16.2 5.48 27.51 10.15 9.27 63.5 26.9718.026.34 3.4Transformer (Pro) 8.67 13.9222.45 9.36 14.21 5.16 19.88 10.66 12.2352.0 30.18 25.538.37 10.72FoX (LLaMA) 9.48 15.5517.13 5.26 15.78 3.78 21.95 10.59 8.63 29.0 19.16 10.07 6.93 9.89Transformer (LLaMA) 8.44 10.08 18.77 6.09 14.47 3.98 11.83 11.5212.9423.5 18.46 16.04 8.27 13.5Samba 6.33 10.89 15.86 5.1 11.28 2.79 9.42 11.39 10.88 28.5 16.07 2.8 11.65 14.26Transformer-SW A (LLaMA) 8.46 8.59 16.65 6.913.84 4.03 7.47 12.87 10.0 12.0 14.92 5.1 16.16 14.22
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (LLaMA), 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (LLaMA), 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Samba, 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (SWA), 16B tokens
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Mamba-2, 16B tokens
Figure 14: Easy mode needle-in-the-haystack analysis for FoX, the Transformer, Mamba-2, Samba,
and the Transformer with sliding window attention. These are 760M-parameter models trained on
16B tokens on LongCrawl64. The results are scored on a scale of 1 (red) to 10 (green) by GPT-4o.
The vertical dashed line indicates the training context length.
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
HGRN2, Easy
1000410072001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
HGRN2, Standard
Figure 15: Needle-in-the-haystack analysis for HGRN2. The results are scored on a scale of 1 (red)
to 10 (green) by GPT-4o. The vertical dashed line indicates the training context length.
26
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
2.2
2.3
2.4
2.5
2.6
2.7
2.8Loss L(i)
0 8k 16k 24k 32k 40k 48k 56k 64k
Validation context length l
9
10
11
12
13
14
15Perplexity P(l)
FoX (Pro)
Transformer (Pro)
FoX (LLaMA)
Transformer (LLaMA)
Mamba-2
HGRN2
DeltaNet
Figure 16: Results with 125M-parameter models trained on roughly 2.7B tokens. ( left) Per-token
loss L(i) at different token position i. ( right) Validation perplexity P(l) over different validation
context length l. The vertical dashed line indicates the training context length. The per-token loss is
typically noisy, so we smooth the curve using a moving average sliding window of 101 tokens. In
this plot 1k = 1024.
F.6 A DDITIONAL RESULTS WITH 125M- PARAM /2.7B- TOKEN , 360M- PARAM /7.5B- TOKEN ,
AND 760M- PARAM /16B- TOKEN TRAINING CONFIGURATIONS
Besides our main results with 760M-parameter model trained on 48B tokens, we also report
per-token loss results with 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760M-
parameter/16B-token training configurations in this section. The hyperparameters used are given in
Appendix B and Table 4. Note that, as mentioned in Appendix B, the learning rates for these experi-
ments are tuned for Transformer (LLaMA) for the 16k training context length setting and transferred
to other models and training context lengths, so the reported results may not be optimal for some
models (e.g., FoX typically prefers higher learning rates than the Transformer).
Per-token loss for different models in the main experiment In Figure 16, Figure 17, and Fig-
ure 18, we show the per-token loss for different models given a training context length of 16k tokens
for the 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760M-parameter/16B-token
training configurations, respectively. These results are consistent with the 760M-parameter/48B-
token results in Figure 2.
Per-token loss for different training context lengths In Figure 19, Figure 20, and Figure 21, we
show the per-token loss for the FoX and Transformer models given different training context lengths
for the 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760M-parameter/16B-token
training configurations, respectively. Consistent with the results in Figure 4.5, we see that the advan-
tages of FoX over the Transformer (1) reduce for larger models and (2) increase for longer training
context lengths.
F.7 S ENSITIVITY OF LENGTH EXTRAPOLATION BEHAVIORS TO HYPERPARAMETERS
This section presents more results on the sensitivity of length extrapolation behaviors to hyperpa-
rameters, in addition to our results in Section 4.3 and Figure 5. Figure 22 and Figure 23 show the
easy-mode and standard-mode needle retrieval results for FoX (Pro) and Transformer (Pro) with dif-
ferent numbers of training tokens and learning rates. Figure 24 shows the corresponding per-token
loss curves. As shown in these results, length extrapolation is sensitive to hyperparameters.
27
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.8
1.9
2.0
2.1
2.2Loss L(i)
0 8k 16k 24k 32k 40k 48k 56k 64k
Validation context length l
6.5
7.0
7.5
8.0
8.5
9.0Perplexity P(l)
FoX (Pro)
Transformer (Pro)
FoX (LLaMA)
Transformer (LLaMA)
Mamba-2
HGRN2
DeltaNet
Figure 17: Results with 360M-parameter models trained on roughly 7.5B tokens. ( left) Per-token
loss L(i) at different token position i. ( right) Validation perplexity P(l) over different validation
context length l. The vertical dashed line indicates the training context length. The per-token loss is
typically noisy, so we smooth the curve using a moving average sliding window of 101 tokens. In
this plot 1k = 1024.
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.60
1.65
1.70
1.75
1.80
1.85
1.90Loss L(i)
0 8k 16k 24k 32k 40k 48k 56k 64k
Validation context length l
5.0
5.5
6.0
6.5
7.0Perplexity P(l)
FoX (Pro)
Transformer (Pro)
FoX (LLaMA)
Transformer (LLaMA)
Mamba-2
HGRN2
DeltaNet
Figure 18: Results with 760M-parameter models trained on roughly 16B tokens. ( left) Per-token
loss L(i) at different token position i. ( right) Validation perplexity P(l) over different validation
context length l. The vertical dashed line indicates the training context length. The per-token loss is
typically noisy, so we smooth the curve using a moving average sliding window of 101 tokens. In
this plot 1k = 1024.
28
Published as a conference paper at ICLR 2025
512 1024 2048 4096 8192 16384 32768
Token index i
2.3
2.4
2.5
2.6
2.7Loss L(i)
125M parameters/2.7B training tokens
Model
FoX (LLaMA)
Transformer (LLaMA)
Training context length
4096
8192
16384
32768
512 1024 2048 4096 8192 16384 32768
Token index i
2.2
2.3
2.4
2.5
2.6Loss L(i)
125M parameters/2.7B training tokens
Model
FoX (Pro)
Transformer (Pro)
Training context length
4096
8192
16384
32768
Figure 19: Per-token loss given different training context lengths for the 125M-parameter/2.7B-
token setting. ( left) Results for the LLaMA models. ( right) Results for the Pro models. At each
token index i, we report the averaged loss over a window of 101 centered at i.
512 1024 2048 4096 8192 16384 32768
Token index i
1.9
2.0
2.1
2.2
2.3Loss L(i)
350M parameters/7.5B training tokens
Model
FoX (LLaMA)
Transformer (LLaMA)
Training context length
4096
8192
16384
32768
512 1024 2048 4096 8192 16384 32768
Token index i
1.9
2.0
2.1Loss L(i)
350M parameters/7.5B training tokens
Model
FoX (Pro)
Transformer (Pro)
Training context length
4096
8192
16384
32768
Figure 20: Per-token loss given different training context lengths for the 350M-parameter/7.5B-
token setting. ( left) Results for the LLaMA models. ( right) Results for the Pro models. At each
token index i, we report the averaged loss over a window of 101 centered at i.
512 1024 2048 4096 8192 16384 32768
Token index i
1.7
1.8
1.9
2.0Loss L(i)
760M parameters/16B training tokens
Model
FoX (LLaMA)
Transformer (LLaMA)
Training context length
4096
8192
16384
32768
512 1024 2048 4096 8192 16384 32768
Token index i
1.65
1.70
1.75
1.80
1.85Loss L(i)
760M parameters/16B training tokens
Model
FoX (Pro)
Transformer (Pro)
Training context length
4096
8192
16384
32768
Figure 21: Per-token loss given different training context lengths for the 760M-parameter/16B token
setting. ( left) Results for the LLaMA models. ( right) Results for the Pro models. At each token
index i, we report the averaged loss over a window of 101 centered at i.
29
Published as a conference paper at ICLR 2025
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), 16B tokens, LR=0.001 FoX (Pro), 16B tokens, LR=0.002
1000 4100 72001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), 48B tokens, LR=0.001
1000 4100 72001030013400165001960022700258002890032000
Document Length
FoX (Pro), 48B tokens, LR=0.002
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), 16B tokens, LR=0.001 Transformer (Pro), 16B tokens, LR=0.002
1000 4100 72001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), 48B tokens, LR=0.001
1000 4100 72001030013400165001960022700258002890032000
Document Length
Transformer (Pro), 48B tokens, LR=0.002
Figure 22: FoX (Pro) and Transformer (Pro) easy mode needle-in-the-haystack results for different
numbers of training tokens and learning rates. The vertical dashed line indicates the training context
length.
30
Published as a conference paper at ICLR 2025
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), 16B tokens, LR=0.001 FoX (Pro), 16B tokens, LR=0.002
1000 4100 72001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
FoX (Pro), 48B tokens, LR=0.001
1000 4100 72001030013400165001960022700258002890032000
Document Length
FoX (Pro), 48B tokens, LR=0.002
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), 16B tokens, LR=0.001 Transformer (Pro), 16B tokens, LR=0.002
1000 4100 72001030013400165001960022700258002890032000
Document Length
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0 Depth Percent
Transformer (Pro), 48B tokens, LR=0.001
1000 4100 72001030013400165001960022700258002890032000
Document Length
Transformer (Pro), 48B tokens, LR=0.002
Figure 23: FoX (Pro) and Transformer (Pro) standard mode needle-in-the-haystack results for dif-
ferent numbers of training tokens and learning rates. The vertical dashed line indicates the training
context length.
31
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.5
1.6
1.7
1.8
1.9Loss L(i)
FoX (Pro), 16B tokens, LR=0.001
FoX (Pro), 16B tokens, LR=0.002
FoX (Pro), 48B tokens, LR=0.001
FoX (Pro), 48B tokens, LR=0.002
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.5
1.6
1.7
1.8
1.9Loss L(i)
Transformer (Pro), 16B tokens, LR=0.001
Transformer (Pro), 16B tokens, LR=0.002
Transformer (Pro), 48B tokens, LR=0.001
Transformer (Pro), 48B tokens, LR=0.002
Figure 24: FoX (Pro) and Transformer (Pro) per-token loss for different numbers of training tokens
and learning rates. The vertical dashed line indicates the training context length.
0 1 2 3 4 5
Training tokens 1e10
1.6
1.8
2.0
2.2
2.4
2.6Loss
FoX (Pro)
FoX (LLaMA)
Transformer (Pro)
Transformer (LLaMA)
Mamba-2
HGRN2
DeltaNet
Figure 25: Training curves of different models presented in Figure 2. These curves show the training
loss averaged every 512 × 220 tokens. All models have 760M parameters.
F.8 T RAINING CURVES
In Figure 25, we show the training curves for all models presented in Figure 2. Note that different
models use different peak learning rates, so their learning curves have different shapes.
F.9 S TABILITY ACROSS RANDOM SEEDS
Although it is computationally impractical for us to run multiple seeds for all our results, we have
run three seeds for our360M-parameter FoX (LLaMA) model to show that the variance across seeds
is small. As shown in Figure 26, the variance across seeds is small.
32
Published as a conference paper at ICLR 2025
0 8k 16k 24k 32k 40k 48k 56k 64k
Token index i
1.9
2.0
2.1
2.2Loss L(i)
FoX (LLaMA, seed 0)
FoX (LLaMA, seed 1)
FoX (LLaMA, seed 2)
Figure 26: Result stability across seeds with360M-parameter FoX (LLaMA). All models are trained
on roughly 7.5B tokens. The vertical dashed line indicates the training context length. The per-token
loss is typically noisy, so we smooth the curve using a moving average sliding window of101 tokens.
In this plot 1k = 1024.
F.10 A DDITIONAL VISUALIZATION OF FORGET GATE AND ATTENTION SCORE MATRICES
In Figure 27 and Figure 28, we show the forget gate matrices F and the attention score matrices A
from 16 heads distributed in 4 layers. Note that since these matrices are large ( 16384 × 16384), if
only near-diagonal entries of F are non-zero the visualization will look almost all black.
33
Published as a conference paper at ICLR 2025
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 1, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 2, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 3, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 4, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 1, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 2, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 3, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 4, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 1, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 2, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 3, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 4, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 1, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 2, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 3, F
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 4, F
0.0 0.2 0.4 0.6 0.8 1.0
Score
Figure 27: Visualization of the forget gate weight matrix F from 16 heads in 4 different layers.
These results use FoX (Pro).
34
Published as a conference paper at ICLR 2025
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 1, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 2, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 3, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 1, Head 4, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 1, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 2, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 3, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 8, Head 4, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 1, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 2, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 3, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 16, Head 4, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 1, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 2, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 3, A
0k 5k 10k 15k
Key Positions
0k
5k
10k
15k Query Positions
Layer 24, Head 4, A
0.0 0.2 0.4 0.6 0.8 1.0
Score
Figure 28: Visualization of the attention score matrix A from 16 heads in 4 different layers. These
results use FoX (Pro). Since A is very sparse, we only show entries larger than 0.1.
35