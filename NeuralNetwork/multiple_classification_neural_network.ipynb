{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.788708Z",
     "start_time": "2025-01-12T04:22:31.758672Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.793637Z",
     "start_time": "2025-01-12T04:22:31.791713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters for a deep neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_dims: (list) - the number of units of each layer in the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dict) with keys where 1 <= l <= len(layer_dims) - 1:\n",
    "        Wl: (ndarray (layer_dims[l], layer_dims[l-1])) - weight matrix for layer l\n",
    "        bl: (ndarray (layer_dims[l], 1)) - bias vector for layer l\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ],
   "id": "301b51ecf249e74b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.846483Z",
     "start_time": "2025-01-12T04:22:31.844841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implements the linear part of a layer's forward propagation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_prev: (ndarray (size of previous layer, number of examples)) - activations from previous layer\n",
    "    W: (ndarray (size of current layer, size of previous layer)) - weight matrix\n",
    "    b: (ndarray (size of current layer, 1)) - bias vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Z: (ndarray (size of current layer, number of examples)) - the input to the activation function\n",
    "    cache: (tuple) - containing A_prev, W, b for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    Z = W @ A_prev + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ],
   "id": "8dcb115c5c5ba8b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.850492Z",
     "start_time": "2025-01-12T04:22:31.848684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    # Subtracting the maximum value in each column for numerical stability to avoid overflow\n",
    "    Z_stable = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    exp_Z = np.exp(Z_stable)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "a35fee37284e5003",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.857953Z",
     "start_time": "2025-01-12T04:22:31.856212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the linear and activation layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_prev: (ndarray (size of previous layer, number of examples)) - activations from previous layer\n",
    "    W: (ndarray (size of current layer, size of previous layer)) - weight matrix\n",
    "    b: (ndarray (size of current layer, 1)) - bias vector\n",
    "    activation_function: (str) - the activation function to be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray (size of current layer, number of examples)) - the output of the activation function\n",
    "    cache: (tuple) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation_function == 'softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation_function == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    else:\n",
    "        raise ValueError(f'Activation function {activation_function} not supported.')\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "597a6b94c155ee1b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.861198Z",
     "start_time": "2025-01-12T04:22:31.859502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_forward(X, parameters, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the entire network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    parameters: (dict) - output of initialize_parameters()\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AL: (ndarray (output size, number of examples)) - the output of the last layer\n",
    "    caches: (list of tuples) - containing caches for each layer\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(activation_functions)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], activation_functions[l])\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ],
   "id": "f02bd45bc8c29c1f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.865978Z",
     "start_time": "2025-01-12T04:22:31.864554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AL: (ndarray (output size, number of examples)) - probability vector corresponding to the label predictions\n",
    "    Y: (ndarray (output size, number of examples)) - true label vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost: (float) - the cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(AL))\n",
    "    return cost"
   ],
   "id": "144863fddf5dab84",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.870855Z",
     "start_time": "2025-01-12T04:22:31.869211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear portion of backward propagation for a single layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dZ: (ndarray (size of current layer, number of examples)) - gradient of the cost with respect to the linear output\n",
    "    cache: (tuple) - containing W, A_prev, b from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation from the previous layer\n",
    "    dW: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W\n",
    "    db: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    dW = dZ @ A_prev.T\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "    return dA_prev, dW, db"
   ],
   "id": "c652b0152d0a76be",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.875997Z",
     "start_time": "2025-01-12T04:22:31.873973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single softmax unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    def softmax_jacobian(Z):\n",
    "        Z_stable = Z - np.max(Z, axis=0, keepdims=True)\n",
    "        exp_Z = np.exp(Z_stable)\n",
    "        g = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "        return np.diag(g) - np.outer(g, g)\n",
    "\n",
    "    Z = cache\n",
    "    m = Z.shape[1]\n",
    "    dZ = np.zeros_like(Z)\n",
    "    for k in range(m):\n",
    "        dZ[:, k] = softmax_jacobian(Z[:, k]) @ dA[:, k]\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ"
   ],
   "id": "149efbd504b9a888",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.880663Z",
     "start_time": "2025-01-12T04:22:31.878945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the linear and activation layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray (size of current layer, number of examples)) - post-activation gradient for current layer\n",
    "    cache: (tuple) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for backpropagation\n",
    "    activation_function: (str) - the activation function to be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation from the previous layer\n",
    "    dW: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W\n",
    "    db: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_function == 'softmax':\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    elif activation_function == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise ValueError(f'Activation function {activation_function} not supported.')\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "d53f35765ac4fa38",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.885926Z",
     "start_time": "2025-01-12T04:22:31.883974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_backward(AL, Y, caches, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the entire network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AL: (ndarray (output size, number of examples)) - the output of the last layer\n",
    "    Y: (ndarray (output size, number of examples)) - true labels\n",
    "    caches: (list of tuples) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for each layer\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: (dict) with keys where 0 <= l <= len(activation_functions) - 1:\n",
    "        dA{l-1}: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation for previous layer l - 1\n",
    "        dWl: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W for layer l\n",
    "        dbl: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b for layer l\n",
    "    \"\"\"\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(activation_functions)\n",
    "    m = AL.shape[1]\n",
    "    dAL = -(1 / m) * (Y / AL)\n",
    "    dA_prev = dAL\n",
    "    for l in reversed(range(1, L)):\n",
    "        current_cache = caches[l - 1]\n",
    "        dA_prev, dW, db = linear_activation_backward(dA_prev, current_cache, activation_functions[l])\n",
    "        gradients[f'dA{l - 1}'] = dA_prev\n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "    return gradients"
   ],
   "id": "d733177c8f2ae259",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.890751Z",
     "start_time": "2025-01-12T04:22:31.888933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: (dict) - containing the parameters\n",
    "    gradients: (dict) - containing the gradients\n",
    "    learning_rate: (float) - the learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params: (dict) - containing the updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    updated_parameters = parameters.copy()\n",
    "    L = len(updated_parameters) // 2\n",
    "    for l in range(L):\n",
    "        updated_parameters[f'W{l + 1}'] = parameters[f'W{l + 1}'] - learning_rate * gradients[f'dW{l + 1}']\n",
    "        updated_parameters[f'b{l + 1}'] = parameters[f'b{l + 1}'] - learning_rate * gradients[f'db{l + 1}']\n",
    "    return updated_parameters"
   ],
   "id": "adb6aa01d21ab4d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.895620Z",
     "start_time": "2025-01-12T04:22:31.893774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_model(X, Y, init_parameters, layer_activation_functions, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Implements a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    Y: (ndarray (output size, number of examples)) - true labels\n",
    "    init_parameters: (dict) - the initial parameters for the network\n",
    "    layer_activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "    learning_rate: (float) - the learning rate\n",
    "    num_iterations: (int) - the number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: (dict) - the learned parameters\n",
    "    costs: (list) - the costs at every 100th iteration\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    parameters = init_parameters.copy()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = model_forward(X, parameters, layer_activation_functions)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        gradients = model_backward(AL, Y, caches, layer_activation_functions)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ],
   "id": "fc03446d54418d66",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:22:31.900143Z",
     "start_time": "2025-01-12T04:22:31.898540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_model_predict(X, parameters, activation_functions):\n",
    "    \"\"\"\n",
    "    Predicts the output of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    parameters: (dict) - the learned parameters\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: (ndarray (number of classes, number of examples)) - the predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    probabilities, _ = model_forward(X, parameters, activation_functions)\n",
    "    pred = np.argmax(probabilities, axis=0)\n",
    "    predictions = np.zeros_like(probabilities)\n",
    "    for i in range(predictions.shape[1]):\n",
    "        predictions[pred[i], i] = 1\n",
    "    return predictions"
   ],
   "id": "1e3095bf36bcd6eb",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
