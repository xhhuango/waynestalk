{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:51.946263Z",
     "start_time": "2025-01-12T04:21:51.917875Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:51.951736Z",
     "start_time": "2025-01-12T04:21:51.949542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters for a deep neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_dims: (list) - the number of units of each layer in the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dict) with keys where 1 <= l <= len(layer_dims) - 1:\n",
    "        Wl: (ndarray (layer_dims[l], layer_dims[l-1])) - weight matrix for layer l\n",
    "        bl: (ndarray (layer_dims[l], 1)) - bias vector for layer l\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ],
   "id": "6f65bbd30827c6c5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.002908Z",
     "start_time": "2025-01-12T04:21:52.001219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implements the linear part of a layer's forward propagation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_prev: (ndarray (size of previous layer, number of examples)) - activations from previous layer\n",
    "    W: (ndarray (size of current layer, size of previous layer)) - weight matrix\n",
    "    b: (ndarray (size of current layer, 1)) - bias vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Z: (ndarray (size of current layer, number of examples)) - the input to the activation function\n",
    "    cache: (tuple) - containing A_prev, W, b for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    Z = W @ A_prev + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ],
   "id": "2342ef3c7f5ab983",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.008714Z",
     "start_time": "2025-01-12T04:21:52.006580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Implements the tanh activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def leaky_relu(Z, negative_slope=0.01):\n",
    "    \"\"\"\n",
    "    Implements the Leaky ReLU activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z: (ndarray of any shape) - input to the activation function\n",
    "    negative_slope: (float) - the slope for negative values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray of same shape as Z) - output of the activation function\n",
    "    cache: (ndarray) - returning Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z) + negative_slope * np.minimum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "c7213a3fb31e0819",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.014053Z",
     "start_time": "2025-01-12T04:21:52.011943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the linear and activation layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_prev: (ndarray (size of previous layer, number of examples)) - activations from previous layer\n",
    "    W: (ndarray (size of current layer, size of previous layer)) - weight matrix\n",
    "    b: (ndarray (size of current layer, 1)) - bias vector\n",
    "    activation_function: (str) - the activation function to be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: (ndarray (size of current layer, number of examples)) - the output of the activation function\n",
    "    cache: (tuple) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation_function == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation_function == 'tanh':\n",
    "        A, activation_cache = tanh(Z)\n",
    "    elif activation_function == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "    else:\n",
    "        raise ValueError(f'Activation function {activation_function} not supported.')\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "6f47bd86ceee08c4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.018875Z",
     "start_time": "2025-01-12T04:21:52.017152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_forward(X, parameters, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the entire network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    parameters: (dict) - output of initialize_parameters()\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AL: (ndarray (output size, number of examples)) - the output of the last layer\n",
    "    caches: (list of tuples) - containing caches for each layer\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(activation_functions)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], activation_functions[l])\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ],
   "id": "324200ca1efcd53",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.023623Z",
     "start_time": "2025-01-12T04:21:52.021975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AL: (ndarray (1, number of examples)) - the output of the last layer\n",
    "    Y: (ndarray (1, number of examples)) - true labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost: (float) - the cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL), axis=1, keepdims=True)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ],
   "id": "26e8dae7ad4e3158",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.028214Z",
     "start_time": "2025-01-12T04:21:52.026629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear portion of backward propagation for a single layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dZ: (ndarray (size of current layer, number of examples)) - gradient of the cost with respect to the linear output\n",
    "    cache: (tuple) - containing W, A_prev, b from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation from the previous layer\n",
    "    dW: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W\n",
    "    db: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    dW = dZ @ A_prev.T\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "    return dA_prev, dW, db"
   ],
   "id": "e5cb517b215f8e5b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.033779Z",
     "start_time": "2025-01-12T04:21:52.031359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single sigmoid unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    g = 1 / (1 + np.exp(-Z))\n",
    "    g_prime = g * (1 - g)\n",
    "    dZ = dA * g_prime\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single tanh unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    g = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    g_prime = 1 - g ** 2\n",
    "    dZ = dA * g_prime\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def leaky_relu_backward(dA, cache, negative_slope=0.01):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a single Leaky ReLU unit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray of any shape) - post-activation gradient\n",
    "    cache: (ndarray) - Z from the forward propagation\n",
    "    negative_slope: (float) - the slope for negative values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dZ: (ndarray of the same shape as A) - gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z < 0] = negative_slope\n",
    "    return dZ"
   ],
   "id": "59a84cdd76ad7a7a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.038818Z",
     "start_time": "2025-01-12T04:21:52.037041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the linear and activation layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dA: (ndarray (size of current layer, number of examples)) - post-activation gradient for current layer\n",
    "    cache: (tuple) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for backpropagation\n",
    "    activation_function: (str) - the activation function to be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation from the previous layer\n",
    "    dW: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W\n",
    "    db: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_function == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif activation_function == 'tanh':\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "    elif activation_function == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        dZ = leaky_relu_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise ValueError(f'Activation function {activation_function} not supported.')\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "f35689dbc56fa9f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.043827Z",
     "start_time": "2025-01-12T04:21:52.041866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_backward(AL, Y, caches, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the entire network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AL: (ndarray (output size, number of examples)) - the output of the last layer\n",
    "    Y: (ndarray (output size, number of examples)) - true labels\n",
    "    caches: (list of tuples) - containing linear_cache (A_prev, W, b) and activation_cache (Z) for each layer\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: (dict) with keys where 0 <= l <= len(activation_functions) - 1:\n",
    "        dA{l-1}: (ndarray (size of previous layer, number of examples)) - gradient of the cost with respect to the activation for previous layer l - 1\n",
    "        dWl: (ndarray (size of current layer, size of previous layer)) - gradient of the cost with respect to W for layer l\n",
    "        dbl: (ndarray (size of current layer, 1)) - gradient of the cost with respect to b for layer l\n",
    "    \"\"\"\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(activation_functions)\n",
    "    m = AL.shape[1]\n",
    "    dAL = -(1 / m) * (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dA_prev = dAL\n",
    "    for l in reversed(range(1, L)):\n",
    "        current_cache = caches[l - 1]\n",
    "        dA_prev, dW, db = linear_activation_backward(dA_prev, current_cache, activation_functions[l])\n",
    "        gradients[f'dA{l - 1}'] = dA_prev\n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "    return gradients"
   ],
   "id": "b2900af9edc34c46",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.048501Z",
     "start_time": "2025-01-12T04:21:52.046847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: (dict) - containing the parameters\n",
    "    gradients: (dict) - containing the gradients\n",
    "    learning_rate: (float) - the learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params: (dict) - containing the updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    updated_parameters = parameters.copy()\n",
    "    L = len(updated_parameters) // 2\n",
    "    for l in range(L):\n",
    "        updated_parameters[f'W{l + 1}'] = parameters[f'W{l + 1}'] - learning_rate * gradients[f'dW{l + 1}']\n",
    "        updated_parameters[f'b{l + 1}'] = parameters[f'b{l + 1}'] - learning_rate * gradients[f'db{l + 1}']\n",
    "    return updated_parameters"
   ],
   "id": "26cb048586428a7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.053535Z",
     "start_time": "2025-01-12T04:21:52.051743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_model(X, Y, init_parameters, layer_activation_functions, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Implements a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    Y: (ndarray (output size, number of examples)) - true labels\n",
    "    init_parameters: (dict) - the initial parameters for the network\n",
    "    layer_activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "    learning_rate: (float) - the learning rate\n",
    "    num_iterations: (int) - the number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: (dict) - the learned parameters\n",
    "    costs: (list) - the costs at every 100th iteration\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    parameters = init_parameters.copy()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = model_forward(X, parameters, layer_activation_functions)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        gradients = model_backward(AL, Y, caches, layer_activation_functions)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ],
   "id": "7486cc9e8128fcb1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:21:52.057899Z",
     "start_time": "2025-01-12T04:21:52.056415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_model_predict(X, parameters, activation_functions):\n",
    "    \"\"\"\n",
    "    Predicts the output of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (ndarray (input size, number of examples)) - input data\n",
    "    parameters: (dict) - the learned parameters\n",
    "    activation_functions: (list) - the activation function for each layer. The first element is unused.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: (ndarray (1, number of examples)) - the predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    probabilities, _ = model_forward(X, parameters, activation_functions)\n",
    "    predictions = probabilities.copy()\n",
    "    predictions[predictions > 0.5] = 1\n",
    "    predictions[predictions <= 0.5] = 0\n",
    "    return predictions"
   ],
   "id": "9566cd1b5320c736",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
